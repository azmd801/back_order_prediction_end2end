{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azmd801/back_order_prediction/blob/main/notebook/Data_transformation_%26_Model_Selection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jD1cEniP4fR"
      },
      "source": [
        "# Abstract\n",
        "\n",
        "In the initial phase, a thorough Exploratory Data Analysis (EDA) was conducted, shedding light on the complex issue of backorders in the retail and supply chain sectors. This detailed examination of a large dataset hinted at the vital role predictive models could play in refining inventory management and enhancing customer satisfaction. The EDA revealed several preprocessing opportunities, including tackling high skewness and managing outliers. It suggested that these preprocessing options could be treated as hyperparameters, allowing for a systematic cross-validation process to identify the best pipeline. This strategy aims to pave the way for a predictive model capable of effectively forecasting backorders, fostering improved management approaches and greater operational efficiency in the relevant industries. [Open EDA Notebook](https://github.com/azmd801/back_order_prediction/blob/main/notebook/eda.ipynb)\n",
        "\n",
        "This notebook presents a systematic approach to data transformation, focusing on the critical role of preprocessing choices treated as hyperparameters. These choices are meticulously optimized through cross-validation to determine the best preprocessing pipeline. Initially, the process begins with the removal of redundant features, such as 'sku.'\n",
        "\n",
        "As the approach unfolds, it explores various preprocessing techniques, including handling outliers, addressing sparse features, and strategies for categorical feature handling and imputation. A detailed plan is established to improve predictability and ensure consistency through robust preprocessing pipelines.\n",
        "\n",
        "Furthermore, custom functions and classes are introduced to simplify the data transformation process, concentrating on the removal of irrelevant features and the mitigation of outlier effects, ultimately preparing the data for effective modeling. The CustomPipeline class is a central component, enhancing flexibility and efficiency by facilitating the creation of diverse machine learning pipelines through parameterization.\n",
        "\n",
        "In addition, the experimentation of various machine learning pipelines is enabled by the CustomPipeline class, serving as a core element of model exploration. Leveraging Optuna for optimization, this phase involves meticulous identification of the best pipeline, leveraging 100,000 samples, which represent 10% of the entire dataset, to ensure robust model selection.\n",
        "\n",
        "While Logistic Regression initially emerges as a model choice, the limited sample size raises questions about its reliability. Consequently, a more detailed evaluation encompasses various classifiers, including XGBClassifier, RandomForestClassifier, and SVC. These classifiers are empowered by transformations determined by Optuna's best pipeline, enhancing their performance across the complete dataset.\n",
        "\n",
        "Subsequent steps involve fine-tuning the selected model's hyperparameters through Optuna, aiming to further enhance accuracy and suitability for the task. The evaluation process considers not only the balanced accuracy score (0.84) but also the ROC AUC score (0.93), which quantifies the model's ability to differentiate products at risk of backorder effectively.\n",
        "\n",
        "Additionally, the exploratory data analysis (EDA) conducted during the preliminary stages of this process revealed key insights into the dataset's characteristics. Notably, it highlighted issues such as data imbalance, outliers in certain features, and sparse distributions in others, providing essential context for subsequent preprocessing and modeling decisions.\n",
        "\n",
        "In the final assessment, the chosen model is rigorously evaluated to determine its effectiveness and identify areas for improvement. While the model excels in identifying most backorders (Test balanced accuracy: 0.84 and Test ROC AUC: 0.93), there's room for optimization, particularly in reducing false positives, as indicated by the test precision score (0.09) and the test F1 score (0.16).\n",
        "\n",
        "The overarching goal is to facilitate proactive management strategies and enhance operational efficiency in the retail and supply chain sectors by fine-tuning the model for high balanced accuracy. This improvement ultimately leads to better resource allocation and increased customer satisfaction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcyP4TOolYt7"
      },
      "source": [
        "# Approcah for data transformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u82caR-iagM7"
      },
      "source": [
        "There are lot of options availble for preprocessing data all these options are hyperparameters and can be cross validated to find the best preprocessing pipelin\n",
        "\n",
        "### Fixed steps which will done initially before any data transformation\n",
        "* dropping unncessary features  `'sku'`\n",
        "\n",
        "### Diffrent pre-processing options availbale\n",
        "\n",
        "1. use all the data as it is without any modification\n",
        "2. Use winsorization to handle outlier\n",
        "3. removing extreme sparse feature  `'in_transit_qty'`, `'local_bo_qty'`, `'pieces_past_due'`\n",
        "4. Removing extreme imbalance features `'potential_issue'`, `'oe_constraint'`, and `'rev_stop'`\n",
        "5. Removing extreme sparse features\n",
        "\n",
        "### Options for Handling categorical feature\n",
        "1. Use one hot encode by removing first dummy variable\n",
        "2.Keeping all the variabbles\n",
        "\n",
        "### Options for imputation\n",
        "\n",
        " * `lead_time` has 100894 missing records (6%) which is large so these records cant be dropped\n",
        " * For rest of the features there is only one missing records so these will be dropped\n",
        " * For handiling the missing value of lead_time knn_imputer and median imputaion will be used\n",
        " * add_indicator will be explored\n",
        "\n",
        "### Refrences\n",
        "\n",
        " * https://chinmaydalvi.medium.com/backorder-prediction-using-machine-learning-cbe2a7d2cfa4\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZAEXyA0va3-"
      },
      "source": [
        "# Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rpq1B819aetL",
        "outputId": "00883657-efc2-47c7-cad5-1d1d5bdb0c78"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'3.3.0'"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Standard library imports\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# External libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List\n",
        "from scipy.stats.mstats import winsorize\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder, StandardScaler, LabelEncoder, PowerTransformer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import (balanced_accuracy_score, roc_auc_score, precision_score, recall_score,\n",
        "                             f1_score, roc_curve, auc, confusion_matrix, make_scorer)\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from imblearn.pipeline import Pipeline as Pipeline\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from tabulate import tabulate\n",
        "import optuna\n",
        "import patoolib\n",
        "\n",
        "# Settings and installations\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "!pip install --quiet unrar\n",
        "!pip install --quiet patool\n",
        "!pip install --quiet optuna\n",
        "\n",
        "# Check optuna version\n",
        "optuna.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WrCbTEdvqoY"
      },
      "source": [
        "# Custom functions and class"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1lywpBiyWkR"
      },
      "source": [
        "##  Feature Transformation Functions\n",
        "\n",
        "In machine learning, creating robust preprocessing pipelines can improve model performance and ensure reproducibility. Several feature transformation functions are introduced here to prepare the data effectively.\n",
        "\n",
        "\n",
        "### `drop_col` Function\n",
        "\n",
        "**Purpose**: To remove irrelevant or unnecessary features.\n",
        "\n",
        "**Rationale:**  \n",
        "- Irrelevant or redundant features can introduce noise and can deteriorate the model's performance.\n",
        "- Simplifying the model by removing such features can potentially enhance its predictability.\n",
        "\n",
        "\n",
        "### `Winsorizer` Class\n",
        "\n",
        "**Purpose**: To mitigate the influence of outliers.\n",
        "\n",
        "**Rationale:**  \n",
        "- Outliers can distort the model's understanding, leading to longer training times and potentially compromising accuracy.\n",
        "- Winsorization is a technique that caps extreme values to a predetermined range, making them less influential and limiting their adverse effect."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g7WvlkkMt6n8"
      },
      "outputs": [],
      "source": [
        "## function to drop columns\n",
        "\n",
        "def drop_col(df: pd.DataFrame, columns: List[str], drop: bool =True) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Drop columns from a DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Input DataFrame.\n",
        "    - columns: List of column names to drop.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame: DataFrame with the specified columns dropped.\n",
        "    \"\"\"\n",
        "    # if 'sku' in df.columns:\n",
        "    #   df.drop('sku',inplace=True,axis=1)\n",
        "\n",
        "    if not drop:\n",
        "      return df.drop('sku',axis=1)\n",
        "\n",
        "    return df.drop('sku',axis=1).drop(columns, axis=1)\n",
        "\n",
        "from sklearn.base import TransformerMixin\n",
        "\n",
        "## custom class with fit and tranform to perform winsorization\n",
        "\n",
        "class Winsorizer(TransformerMixin):\n",
        "    def __init__(self, change=True):\n",
        "        \"\"\"\n",
        "        Initialize the Winsorizer transformer.\n",
        "\n",
        "        Parameters:\n",
        "        - lower_quantile (float): Lower quantile for winsorization (default: 0.05).\n",
        "        - upper_quantile (float): Upper quantile for winsorization (default: 0.95).\n",
        "        \"\"\"\n",
        "        self.change = change\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        \"\"\"\n",
        "        Fit the Winsorizer transformer.\n",
        "\n",
        "        Parameters:\n",
        "        - X (array-like): Input data.\n",
        "        - y: Ignored.\n",
        "\n",
        "        Returns:\n",
        "        - self: Returns the instance of the transformer.\n",
        "        \"\"\"\n",
        "        # Calculate the percentiles\n",
        "        p0 = np.nanpercentile(X, 0)\n",
        "        p100 = np.nanpercentile(X, 100)\n",
        "\n",
        "        # Calculate the lower and upper IQR\n",
        "        Q1 = np.nanpercentile(X, 25)\n",
        "        Q3 = np.nanpercentile(X, 75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        # Calculate the lower and upper bounds\n",
        "        self.lower_bound = max(Q1 - (1.5 * IQR),p0)\n",
        "        self.upper_bound = min(Q3 + (1.5 * IQR),p100)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        \"\"\"\n",
        "        Transform the input data using winsorization.\n",
        "\n",
        "        Parameters:\n",
        "        - X (array-like): Input data to be transformed.\n",
        "\n",
        "        Returns:\n",
        "        - X_transformed (array-like): Transformed data after winsorization.\n",
        "        \"\"\"\n",
        "\n",
        "        if not self.change:\n",
        "          return X\n",
        "\n",
        "        X_clipped = np.clip(X, self.lower_bound, self.upper_bound)\n",
        "        return X_clipped\n",
        "\n",
        "    def get_feature_names_out(self, input_features):\n",
        "        \"\"\"\n",
        "        Get the feature names after transformation.\n",
        "\n",
        "        Parameters:\n",
        "        - input_features (array-like): Input feature names.\n",
        "\n",
        "        Returns:\n",
        "        - output_features (array-like): Transformed feature names.\n",
        "        \"\"\"\n",
        "        return input_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HqdbtUY1THZ"
      },
      "source": [
        "## CustomMLPipeline Class\n",
        "\n",
        "The `CustomMLPipeline` class facilitates the construction of a machine learning pipeline specifically tailored for classification. By incorporating various preprocessing, feature engineering, and modeling techniques, this class enables users to create and experiment with different pipeline configurations efficiently.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99lYWwnqHfnW"
      },
      "outputs": [],
      "source": [
        "class CustomMLPipeline:\n",
        "    \"\"\"\n",
        "    Custom Machine Learning Pipeline for Classification.\n",
        "\n",
        "    This class provides tools for creating a tailored machine learning pipeline, designed to streamline the\n",
        "    process of experimenting with different configurations and finding the optimal setup for classification tasks.\n",
        "\n",
        "    Attributes:\n",
        "    - imputer_type (str): Choose an imputation method for handling missing data.\n",
        "    - drop_and_winsorize (bool): Enable dropping of columns and applying winsorization.\n",
        "    - classifier (str): Select a classifier for training the model.\n",
        "    - columns_to_drop (str): Specify columns to drop from the data.\n",
        "    - add_indicator (bool): Indicate whether to add informative indicator columns.\n",
        "    - keep_first (bool): Specify whether to retain the first category during encoding.\n",
        "\n",
        "    Methods:\n",
        "    - __init__(*args, add_indicator: bool = False, keep_first: bool = False):\n",
        "        Initialize the custom classification pipeline.\n",
        "\n",
        "    - _create_classifier() -> Classifier:\n",
        "        Create and return an instance of the chosen classifier.\n",
        "\n",
        "    - _drop_col(X: pd.DataFrame) -> pd.DataFrame:\n",
        "        Drop specified columns from the input DataFrame.\n",
        "\n",
        "    - create_pipeline() -> Pipeline:\n",
        "        Construct and return a customized classification pipeline.\n",
        "\n",
        "    By adjusting the parameters and options, you can systematically explore various approaches to preprocessing,\n",
        "    classification, and feature engineering. This class empowers you to iterate and fine-tune your pipeline to\n",
        "    discover the most effective way to teach your model to classify data.\n",
        "    \"\"\"\n",
        "    # Common varaibles used for pipeline exprementation defined as  class-level attributes\n",
        "    useless_cols = ['in_transit_qty', 'local_bo_qty', 'pieces_past_due',\n",
        "                'potential_issue', 'oe_constraint', 'rev_stop']\n",
        "\n",
        "    sparse_cols = ['in_transit_qty', 'forecast_3_month', 'forecast_6_month',\n",
        "                  'forecast_9_month', 'sales_1_month', 'sales_3_month',\n",
        "                  'sales_6_month', 'sales_9_month', 'min_bank',\n",
        "                  'pieces_past_due', 'local_bo_qty']\n",
        "\n",
        "    numeric_cols = ['national_inv', 'lead_time', 'in_transit_qty',\n",
        "                    'forecast_3_month', 'forecast_6_month', 'forecast_9_month',\n",
        "                    'sales_1_month', 'sales_3_month', 'sales_6_month',\n",
        "                    'sales_9_month', 'min_bank', 'pieces_past_due',\n",
        "                    'perf_6_month_avg', 'perf_12_month_avg', 'local_bo_qty']\n",
        "\n",
        "    cat_cols = ['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk',\n",
        "                'stop_auto_buy', 'rev_stop']\n",
        "\n",
        "    def __init__(self, imputer_type, drop_and_winsorize, classifier, columns_to_drop,\n",
        "                 add_indicator=False, keep_first=False):\n",
        "        \"\"\"\n",
        "        Initialize the custom classification pipeline.\n",
        "\n",
        "        Parameters:\n",
        "        - imputer_type (str): Type of imputation strategy for handling missing data.\n",
        "        - drop_and_winsorize (bool): Flag indicating whether to drop columns and perform winsorization.\n",
        "        - classifier (str): Type of classifier to be used for the pipeline.\n",
        "        - columns_to_drop (str): Type of columns to be dropped from the dataset.\n",
        "        - add_indicator (bool, optional): Include additional indicator columns after imputation. Default is False.\n",
        "        - keep_first (bool, optional): Keep the first category during feature encoding. Default is False.\n",
        "        \"\"\"\n",
        "        self.imputer_type = imputer_type\n",
        "        self.drop_and_winsorize = drop_and_winsorize\n",
        "        self.classifier = classifier\n",
        "        self.columns_to_drop = columns_to_drop\n",
        "        self.add_indicator = add_indicator\n",
        "        self.keep_first = keep_first\n",
        "\n",
        "\n",
        "        # Determine columns to drop based on configuration\n",
        "        if self.drop_and_winsorize:\n",
        "            self.drop_cols = self.useless_cols if self.columns_to_drop == 'useless' else self.sparse_cols\n",
        "        else:\n",
        "            self.drop_cols = []\n",
        "\n",
        "        # Columns to be used in the pipeline\n",
        "        self.num_cols_used = [col for col in self.numeric_cols if col not in self.drop_cols]\n",
        "        self.cat_cols_used = [col for col in self.cat_cols if col not in self.drop_cols]\n",
        "\n",
        "    def _create_classifier(self):\n",
        "        \"\"\"\n",
        "        Create and return an instance of the chosen classifier.\n",
        "\n",
        "        Returns:\n",
        "        - clf: The chosen classifier instance.\n",
        "        \"\"\"\n",
        "        if self.classifier == 'RandomForest':\n",
        "            clf = RandomForestClassifier()\n",
        "\n",
        "        elif self.classifier == 'SVC':\n",
        "            clf = SVC()\n",
        "\n",
        "        elif self.classifier == 'LogisticRegression':\n",
        "            clf = LogisticRegression(solver='liblinear')\n",
        "\n",
        "        elif self.classifier == 'XGBoost':\n",
        "            clf = XGBClassifier()\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid classifier: {self.classifier}\")\n",
        "\n",
        "        return clf\n",
        "\n",
        "    def _drop_col(self, X):\n",
        "        \"\"\"\n",
        "        Drop specified columns from the input DataFrame.\n",
        "\n",
        "        Parameters:\n",
        "        - X (pd.DataFrame): Input DataFrame.\n",
        "\n",
        "        Returns:\n",
        "        - X_dropped (pd.DataFrame): DataFrame with specified columns removed.\n",
        "        \"\"\"\n",
        "        if not self.drop_and_winsorize:\n",
        "            # Always drop 'sku' column and additional columns specified in drop_cols\n",
        "            X.drop('sku', axis=1)\n",
        "\n",
        "        columns_to_drop = ['sku'] + self.drop_cols\n",
        "        return X.drop(columns_to_drop, axis=1)\n",
        "\n",
        "    def create_pipeline(self):\n",
        "        \"\"\"\n",
        "        Construct and return a customized classification pipeline.\n",
        "\n",
        "        Returns:\n",
        "        - training_pipeline: A tailored pipeline for classification.\n",
        "        \"\"\"\n",
        "        # Conditional creation of imputer based on 'imputer_type'\n",
        "        if self.imputer_type == 'knn':\n",
        "            imp = KNNImputer(weights='distance', add_indicator=self.add_indicator)\n",
        "        else:\n",
        "            imp = SimpleImputer(strategy='median', add_indicator=self.add_indicator)\n",
        "\n",
        "        # Conditional initialization of cat_encoder based on 'keep_first'\n",
        "        cat_encoder = OneHotEncoder(drop='first') if self.keep_first else OneHotEncoder()\n",
        "\n",
        "        # Constructing the numerical pipeline\n",
        "        num_pipeline = Pipeline([\n",
        "            ('scaler', StandardScaler()),\n",
        "            ('imputer', imp),\n",
        "            ('outlier_clipping', Winsorizer(change=self.drop_and_winsorize)),\n",
        "        ])\n",
        "\n",
        "        # Constructing the categorical pipeline\n",
        "        cat_pipeline = Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('onehotencoder', cat_encoder),\n",
        "        ])\n",
        "\n",
        "        # Constructing the training pipeline\n",
        "        clf = self._create_classifier()\n",
        "        training_pipeline = Pipeline([\n",
        "            ('Drop_Columns', FunctionTransformer(self._drop_col)),\n",
        "            ('Balancing', RandomOverSampler()),\n",
        "            ('Feature_transform', ColumnTransformer([\n",
        "                ('num_pipeline', num_pipeline, self.num_cols_used),\n",
        "                ('cat_pipeline', cat_pipeline, self.cat_cols_used),\n",
        "            ])),\n",
        "            ('model', clf)\n",
        "        ])\n",
        "\n",
        "        return training_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKXqeezQ15yl"
      },
      "source": [
        "## `evaluate_and_plot` Function\n",
        "\n",
        "The `evaluate_and_plot` function is designed to evaluate a model's performance on training and validation (or test) data. It computes various performance metrics and displays the ROC curve to provide a visual representation of the model's ability to distinguish between the classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-rKcwLRO3PBy"
      },
      "outputs": [],
      "source": [
        "def evaluate_and_plot(y_train: np.array, y_val: np.array, y_train_pred: np.array,\n",
        "                      y_train_pred_proba: np.array, y_val_pred: np.array, y_val_pred_proba: np.array) -> None:\n",
        "    \"\"\"\n",
        "    Evaluate the model's performance on training and validation (or test) data\n",
        "    and plot the ROC curve for both datasets.\n",
        "\n",
        "    Parameters:\n",
        "    - y_train (np.array): True labels of the training dataset.\n",
        "    - y_val (np.array): True labels of the validation/test dataset.\n",
        "    - y_train_pred (np.array): Predicted labels of the training dataset.\n",
        "    - y_train_pred_proba (np.array): Predicted probabilities of the positive class for the training dataset.\n",
        "    - y_val_pred (np.array): Predicted labels of the validation/test dataset.\n",
        "    - y_val_pred_proba (np.array): Predicted probabilities of the positive class for the validation/test dataset.\n",
        "\n",
        "    Returns:\n",
        "    - None: This function only prints the evaluation metrics and displays the ROC plot.\n",
        "    \"\"\"\n",
        "\n",
        "    # Collecting model performance metrics\n",
        "    performance_metric = [balanced_accuracy_score, roc_auc_score, precision_score, recall_score, f1_score]\n",
        "    report = []\n",
        "\n",
        "    # Iterating over each metric and computing scores for both training and validation data\n",
        "    for metric in performance_metric:\n",
        "        if metric == roc_auc_score:\n",
        "            train_score = metric(y_train, y_train_pred_proba)\n",
        "            val_score = metric(y_val, y_val_pred_proba)\n",
        "        else:\n",
        "            train_score = metric(y_train, y_train_pred)\n",
        "            val_score = metric(y_val, y_val_pred)\n",
        "\n",
        "        report.append([metric.__name__, f'{train_score:.2f}', f'{val_score:.2f}'])\n",
        "\n",
        "    # Printing the evaluation report in a table format\n",
        "    report_table = tabulate(report, headers=[\"Metric\", \"Train Score\", \"Test Score\"], tablefmt=\"pretty\")\n",
        "    print('Model evaluation report:\\n')\n",
        "    print(report_table)\n",
        "\n",
        "    # Generating and printing the normalized confusion matrix for validation data\n",
        "    conf_matrix = confusion_matrix(y_val, y_val_pred, normalize='true') * 100\n",
        "    headers = [\"\", \"Predicted Negative\", \"Predicted Positive\"]\n",
        "    rows = [\n",
        "        [\"Actual Negative\", f\"TNR = {conf_matrix[0][0]:.2f}\", f\"FPR = {conf_matrix[0][1]:.2f}\"],\n",
        "        [\"Actual Positive\", f\"FNR = {conf_matrix[1][0]:.2f}\", f\"TPR = {conf_matrix[1][1]:.2f}\"]\n",
        "    ]\n",
        "    print('\\nValidation Confusion matrix:\\n')\n",
        "    print(tabulate(rows, headers=headers, tablefmt=\"pretty\"))\n",
        "    print('\\n')\n",
        "\n",
        "    # ROC Curve metrics computation\n",
        "    fpr_val, tpr_val, _ = roc_curve(y_val, y_val_pred_proba)\n",
        "    auc_val = auc(fpr_val, tpr_val)\n",
        "\n",
        "    fpr_train, tpr_train, _ = roc_curve(y_train, y_train_pred_proba)\n",
        "    auc_train = auc(fpr_train, tpr_train)\n",
        "\n",
        "    # Plotting ROC curves for both training and validation data\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr_train, tpr_train, color='green', label=f'Train AUC = {auc_train:.2f}')\n",
        "    plt.plot(fpr_val, tpr_val, color='blue', label=f'Test AUC = {auc_val:.2f}')\n",
        "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve - Train & Test')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Qanj-wuZAYP"
      },
      "source": [
        "# Data Ingestion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "Ji7vGgK7WtlH",
        "outputId": "4c7a8979-f1c5-45b0-9554-d6224dd51556"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-09-01 16:43:39--  https://github.com/rodrigosantis1/backorder_prediction/raw/master/dataset.rar\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/rodrigosantis1/backorder_prediction/master/dataset.rar [following]\n",
            "--2023-09-01 16:43:40--  https://raw.githubusercontent.com/rodrigosantis1/backorder_prediction/master/dataset.rar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 24741696 (24M) [application/octet-stream]\n",
            "Saving to: ‘dataset/dataset.rar’\n",
            "\n",
            "dataset/dataset.rar 100%[===================>]  23.59M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2023-09-01 16:43:40 (187 MB/s) - ‘dataset/dataset.rar’ saved [24741696/24741696]\n",
            "\n",
            "patool: Extracting dataset/dataset.rar ...\n",
            "patool: running /usr/bin/unrar x -- /content/dataset/dataset.rar\n",
            "patool:     with cwd='dataset'\n"
          ]
        },
        {
          "ename": "PatoolError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mPatoolError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-bfb6c812e9d4>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Extract the RAR file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mpatoolib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dataset/dataset.rar\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dataset\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patoolib/__init__.py\u001b[0m in \u001b[0;36mextract_archive\u001b[0;34m(archive, verbosity, outdir, program, interactive)\u001b[0m\n\u001b[1;32m    682\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbosity\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracting %s ...\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_extract_archive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprogram\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprogram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patoolib/__init__.py\u001b[0m in \u001b[0;36m_extract_archive\u001b[0;34m(archive, verbosity, interactive, outdir, program, format, compression)\u001b[0m\n\u001b[1;32m    482\u001b[0m             \u001b[0;31m# already handled the command (eg. when it's a builtin Python\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m             \u001b[0;31m# function)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mrun_archive_cmdlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdo_cleanup_outdir\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcleanup_outdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patoolib/__init__.py\u001b[0m in \u001b[0;36mrun_archive_cmdlist\u001b[0;34m(archive_cmdlist, verbosity)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mcmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marchive_cmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_checked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmdlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbosity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbosity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mrunkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/patoolib/util.py\u001b[0m in \u001b[0;36mrun_checked\u001b[0;34m(cmd, ret_ok, **kwargs)\u001b[0m\n\u001b[1;32m    225\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretcode\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mret_ok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Command `%s' returned non-zero exit status %d\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mPatoolError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mretcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mPatoolError\u001b[0m: Command `['/usr/bin/unrar', 'x', '--', '/content/dataset/dataset.rar']' returned non-zero exit status 255"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create a directory to store the dataset\n",
        "if not os.path.exists(\"dataset\"):\n",
        "    os.makedirs(\"dataset\")\n",
        "\n",
        "# Download the RAR file from GitHub\n",
        "!wget -O dataset/dataset.rar https://github.com/rodrigosantis1/backorder_prediction/raw/master/dataset.rar\n",
        "\n",
        "# Extract the RAR file\n",
        "patoolib.extract_archive(\"dataset/dataset.rar\", outdir=\"dataset\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_JU4LKkpFSX",
        "outputId": "ac6e5ba0-7fe4-4070-c8c7-7f2e247f6f82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1687861, 23)"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# reading th data\n",
        "raw_data = pd.read_csv(r\"/content/dataset/Kaggle_Training_Dataset_v2.csv\")\n",
        "raw_data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "id": "2efQtjdnZZkO",
        "outputId": "6bc26aab-bd6f-4f84-db59-86712e7d2f46"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-761af413-c346-468b-9e3d-023e8bb577ae\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sku</th>\n",
              "      <th>national_inv</th>\n",
              "      <th>lead_time</th>\n",
              "      <th>in_transit_qty</th>\n",
              "      <th>forecast_3_month</th>\n",
              "      <th>forecast_6_month</th>\n",
              "      <th>forecast_9_month</th>\n",
              "      <th>sales_1_month</th>\n",
              "      <th>sales_3_month</th>\n",
              "      <th>sales_6_month</th>\n",
              "      <th>...</th>\n",
              "      <th>pieces_past_due</th>\n",
              "      <th>perf_6_month_avg</th>\n",
              "      <th>perf_12_month_avg</th>\n",
              "      <th>local_bo_qty</th>\n",
              "      <th>deck_risk</th>\n",
              "      <th>oe_constraint</th>\n",
              "      <th>ppap_risk</th>\n",
              "      <th>stop_auto_buy</th>\n",
              "      <th>rev_stop</th>\n",
              "      <th>went_on_backorder</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1026827</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1043384</td>\n",
              "      <td>2.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.99</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1043696</td>\n",
              "      <td>2.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1043852</td>\n",
              "      <td>7.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.13</td>\n",
              "      <td>0.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1044048</td>\n",
              "      <td>8.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687856</th>\n",
              "      <td>1373987</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>-99.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687857</th>\n",
              "      <td>1524346</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.84</td>\n",
              "      <td>1.0</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687858</th>\n",
              "      <td>1439563</td>\n",
              "      <td>62.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>39.0</td>\n",
              "      <td>87.0</td>\n",
              "      <td>126.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>63.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.86</td>\n",
              "      <td>0.84</td>\n",
              "      <td>6.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687859</th>\n",
              "      <td>1502009</td>\n",
              "      <td>19.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.73</td>\n",
              "      <td>0.78</td>\n",
              "      <td>1.0</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "      <td>Yes</td>\n",
              "      <td>No</td>\n",
              "      <td>No</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687860</th>\n",
              "      <td>(1687860 rows)</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1687861 rows × 23 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-761af413-c346-468b-9e3d-023e8bb577ae')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-761af413-c346-468b-9e3d-023e8bb577ae button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-761af413-c346-468b-9e3d-023e8bb577ae');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-13dc9c88-e8c0-449c-bb05-cd58bf4b2fc1\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-13dc9c88-e8c0-449c-bb05-cd58bf4b2fc1')\"\n",
              "            title=\"Suggest charts.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const charts = await google.colab.kernel.invokeFunction(\n",
              "          'suggestCharts', [key], {});\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-13dc9c88-e8c0-449c-bb05-cd58bf4b2fc1 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                    sku  national_inv  lead_time  in_transit_qty  \\\n",
              "0               1026827           0.0        NaN             0.0   \n",
              "1               1043384           2.0        9.0             0.0   \n",
              "2               1043696           2.0        NaN             0.0   \n",
              "3               1043852           7.0        8.0             0.0   \n",
              "4               1044048           8.0        NaN             0.0   \n",
              "...                 ...           ...        ...             ...   \n",
              "1687856         1373987          -1.0        NaN             0.0   \n",
              "1687857         1524346          -1.0        9.0             0.0   \n",
              "1687858         1439563          62.0        9.0            16.0   \n",
              "1687859         1502009          19.0        4.0             0.0   \n",
              "1687860  (1687860 rows)           NaN        NaN             NaN   \n",
              "\n",
              "         forecast_3_month  forecast_6_month  forecast_9_month  sales_1_month  \\\n",
              "0                     0.0               0.0               0.0            0.0   \n",
              "1                     0.0               0.0               0.0            0.0   \n",
              "2                     0.0               0.0               0.0            0.0   \n",
              "3                     0.0               0.0               0.0            0.0   \n",
              "4                     0.0               0.0               0.0            0.0   \n",
              "...                   ...               ...               ...            ...   \n",
              "1687856               5.0               7.0               9.0            1.0   \n",
              "1687857               7.0               9.0              11.0            0.0   \n",
              "1687858              39.0              87.0             126.0           35.0   \n",
              "1687859               0.0               0.0               0.0            2.0   \n",
              "1687860               NaN               NaN               NaN            NaN   \n",
              "\n",
              "         sales_3_month  sales_6_month  ...  pieces_past_due  perf_6_month_avg  \\\n",
              "0                  0.0            0.0  ...              0.0            -99.00   \n",
              "1                  0.0            0.0  ...              0.0              0.99   \n",
              "2                  0.0            0.0  ...              0.0            -99.00   \n",
              "3                  0.0            0.0  ...              0.0              0.10   \n",
              "4                  0.0            0.0  ...              0.0            -99.00   \n",
              "...                ...            ...  ...              ...               ...   \n",
              "1687856            3.0            3.0  ...              0.0            -99.00   \n",
              "1687857            8.0           11.0  ...              0.0              0.86   \n",
              "1687858           63.0          153.0  ...              0.0              0.86   \n",
              "1687859            7.0           12.0  ...              0.0              0.73   \n",
              "1687860            NaN            NaN  ...              NaN               NaN   \n",
              "\n",
              "        perf_12_month_avg  local_bo_qty  deck_risk  oe_constraint  ppap_risk  \\\n",
              "0                  -99.00           0.0         No             No         No   \n",
              "1                    0.99           0.0         No             No         No   \n",
              "2                  -99.00           0.0        Yes             No         No   \n",
              "3                    0.13           0.0         No             No         No   \n",
              "4                  -99.00           0.0        Yes             No         No   \n",
              "...                   ...           ...        ...            ...        ...   \n",
              "1687856            -99.00           1.0         No             No         No   \n",
              "1687857              0.84           1.0        Yes             No         No   \n",
              "1687858              0.84           6.0         No             No         No   \n",
              "1687859              0.78           1.0         No             No         No   \n",
              "1687860               NaN           NaN        NaN            NaN        NaN   \n",
              "\n",
              "        stop_auto_buy rev_stop went_on_backorder  \n",
              "0                 Yes       No                No  \n",
              "1                 Yes       No                No  \n",
              "2                 Yes       No                No  \n",
              "3                 Yes       No                No  \n",
              "4                 Yes       No                No  \n",
              "...               ...      ...               ...  \n",
              "1687856           Yes       No                No  \n",
              "1687857            No       No               Yes  \n",
              "1687858           Yes       No                No  \n",
              "1687859           Yes       No                No  \n",
              "1687860           NaN      NaN               NaN  \n",
              "\n",
              "[1687861 rows x 23 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTKRddsdAkr4"
      },
      "source": [
        "# Data cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_zy3AIJxY-8",
        "outputId": "717682a5-fe97-4fa0-b845-c4894ab843dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of data before dropping na (1687861, 23)\n",
            "\n",
            "Shape of data after dropping na (1687860, 23)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# dropping the na record for all the features except for feature 'lead_time'\n",
        "\n",
        "print(f\"Shape of data before dropping na {raw_data.shape}\\n\")\n",
        "raw_data.dropna(subset=raw_data.columns.drop('lead_time'),inplace=True)\n",
        "print(f\"Shape of data after dropping na {raw_data.shape}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HY5O4EyvAttu"
      },
      "source": [
        "# Expermentin with different ML training pipeline using optuna   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g727vQzMp1pz"
      },
      "source": [
        "## Preparing required Variables for optimizing the objective function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFalZNzmu_sD",
        "outputId": "f8ce0045-0958-4b06-b3a7-1463d782f247"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "sku                   object\n",
              "national_inv         float64\n",
              "lead_time            float64\n",
              "in_transit_qty       float64\n",
              "forecast_3_month     float64\n",
              "forecast_6_month     float64\n",
              "forecast_9_month     float64\n",
              "sales_1_month        float64\n",
              "sales_3_month        float64\n",
              "sales_6_month        float64\n",
              "sales_9_month        float64\n",
              "min_bank             float64\n",
              "potential_issue       object\n",
              "pieces_past_due      float64\n",
              "perf_6_month_avg     float64\n",
              "perf_12_month_avg    float64\n",
              "local_bo_qty         float64\n",
              "deck_risk             object\n",
              "oe_constraint         object\n",
              "ppap_risk             object\n",
              "stop_auto_buy         object\n",
              "rev_stop              object\n",
              "went_on_backorder     object\n",
              "dtype: object"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "raw_data.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FtO8gr1EfMaz",
        "outputId": "e479b03a-88d3-4c67-9754-666f8504d801"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(100000, 22)\n",
            "(100000,)\n"
          ]
        }
      ],
      "source": [
        "# Take 10000 random samples from the dataset while stratifying based on a specific column (e.g., 'label')\n",
        "# sample_size = 100000\n",
        "# samples = raw_data.sample(n=sample_size, replace=False)\n",
        "# samples.head()\n",
        "\n",
        "# # spliting X_train and y_train\n",
        "# y_train = LabelEncoder().fit_transform(samples['went_on_backorder'])\n",
        "# X_train = samples.drop('went_on_backorder',axis=1)\n",
        "\n",
        "# print(X_train.shape)\n",
        "# print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTP5cmhm33XE",
        "outputId": "be6b88a5-947e-4f90-9ffc-fa22ae8452d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1687860, 22)\n",
            "(1687860,)\n"
          ]
        }
      ],
      "source": [
        "# spliting X_train and y_train\n",
        "y_train = LabelEncoder().fit_transform(raw_data['went_on_backorder'])\n",
        "X_train = raw_data.drop('went_on_backorder',axis=1)\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dee0co8ZpyXE",
        "outputId": "7aeabe61-f8f2-43d6-9fc9-72333c02262c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0, 0, 0, ..., 1, 0, 0])"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMRkDXFylhJ4",
        "outputId": "0a2e9069-168c-4a83-f5ce-ba356825d805"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Numeric columns: Index(['national_inv', 'lead_time', 'in_transit_qty', 'forecast_3_month',\n",
            "       'forecast_6_month', 'forecast_9_month', 'sales_1_month',\n",
            "       'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank',\n",
            "       'pieces_past_due', 'perf_6_month_avg', 'perf_12_month_avg',\n",
            "       'local_bo_qty'],\n",
            "      dtype='object')\n",
            "\n",
            "Categorical columns: Index(['potential_issue', 'deck_risk', 'oe_constraint', 'ppap_risk',\n",
            "       'stop_auto_buy', 'rev_stop'],\n",
            "      dtype='object')\n",
            "\n",
            "Target columns : went_on_backorder\n",
            "\n",
            "Useless columns: ['in_transit_qty', 'local_bo_qty', 'pieces_past_due', 'potential_issue', 'oe_constraint', 'rev_stop']\n",
            "\n",
            "Sparse columns: ['in_transit_qty', 'forecast_3_month', 'forecast_6_month', 'forecast_9_month', 'sales_1_month', 'sales_3_month', 'sales_6_month', 'sales_9_month', 'min_bank', 'pieces_past_due', 'local_bo_qty']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Identify numeric columns\n",
        "numeric_cols = X_train.columns[X_train.dtypes != 'object']\n",
        "print(f'Numeric columns: {numeric_cols}\\n')\n",
        "\n",
        "# Identify categorical columns, excluding 'sku'\n",
        "cat_cols = X_train.columns[X_train.dtypes == 'object'].drop('sku')\n",
        "print(f'Categorical columns: {cat_cols}\\n')\n",
        "\n",
        "# Mention target column\n",
        "print(f'Target columns : went_on_backorder\\n')\n",
        "\n",
        "# Define columns not needed for analysis\n",
        "useless_cols = ['in_transit_qty', 'local_bo_qty', 'pieces_past_due', 'potential_issue', 'oe_constraint', 'rev_stop']\n",
        "print(f'Useless columns: {useless_cols}\\n')\n",
        "\n",
        "# Calculate proportion of zeros per column\n",
        "zero_proportion = (X_train == 0).sum() / len(X_train)\n",
        "\n",
        "# Drop columns with more than 30% zeros\n",
        "filtered_data = X_train.drop(columns=zero_proportion[zero_proportion < 0.3].index)\n",
        "\n",
        "# List columns with a high number of zeros\n",
        "sparse_cols = filtered_data.columns.tolist()\n",
        "print(f'Sparse columns: {sparse_cols}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcxk8rkDMFQT"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fFCDVenW9E4D"
      },
      "outputs": [],
      "source": [
        "# Calculate the proportion of zeros in each column\n",
        "zero_proportion = (X_train == 0).sum() / len(X_train)\n",
        "\n",
        "# Filter features with more than 30 percent zeros\n",
        "threshold = 0.3\n",
        "filtered_data = X_train.drop(columns=zero_proportion[zero_proportion < threshold].index)\n",
        "\n",
        "# Get the feature names of the remaining columns\n",
        "sparse_cols = filtered_data.columns.tolist()\n",
        "\n",
        "print(sparse_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j01ypXCTixdP",
        "outputId": "ef009b65-5732-4a41-a0d4-36d5b1f4b846"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of X_train: (1012716, 22)\n",
            "Shape of y_train: (1012716,)\n",
            "Shape of X_val: (675145, 22)\n",
            "Shape of y_val: (675145,)\n"
          ]
        }
      ],
      "source": [
        "# Split the data into train and validation sets (80% train, 20% validation)\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=42)\n",
        "print(\"Shape of X_train:\", X_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of X_val:\", X_val.shape)\n",
        "print(\"Shape of y_val:\", y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX5dnFPyqmQ_"
      },
      "source": [
        "## ML pipeline Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--EqyFAbdgXC"
      },
      "source": [
        "### ML Pipeline Selection and Optimization with Optuna\n",
        "\n",
        "In this section, the notebook details the process of selecting and optimizing a machine learning pipeline using the Optuna library. The key components of this section are:\n",
        "\n",
        "- **CustomMLPipeline Class**: This class is used to create a parametrized pipeline, facilitating the integration of various preprocessing, feature engineering, and modeling techniques.\n",
        "\n",
        "- **Objective Function**: In this function, the `CustomMLPipeline` class has been utilized to create a parametrized setup. The optimization of this objective function aims to identify the best parameters, which, in turn, assist in constructing a machine learning pipeline with the highest balanced accuracy. This function comprises different elements that are being optimized, including classifier selection, imputation strategies, categorical feature handling, and outlier management.\n",
        "\n",
        "- **Sampling Strategy**: Due to resource constraints, a subset of 100,000 samples, or about 10% of the total data, is used in the optimization process to find a viable pipeline configuration.\n",
        "\n",
        "- **Pipeline Construction**: This involves setting up a pipeline with different stages such as data preprocessing, feature transformation, and model training.\n",
        "\n",
        "- **Initial Model Selection with Optuna**: At this juncture, Optuna suggests Logistic Regression as the initial model. However, the small sample size used for the optimization casts doubt on the reliability of this model.\n",
        "\n",
        "- **Cross-Validation for Model Evaluation**: To reinforce the model selection process, a cross-validation approach is planned. This method intends to apply the transformations derived from Optuna's best pipeline to the entire dataset, with the goal of finding the most suitable hyperparameters across different models.\n",
        "\n",
        "This section outlines a structured approach to developing and optimizing a machine learning pipeline, with a significant emphasis on achieving the best balanced accuracy through the optimization of the objective function using the `CustomMLPipeline` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4kx0gLGuf_BA"
      },
      "outputs": [],
      "source": [
        "# Defining Objective function which will be optimized\n",
        "\n",
        "def objective(trial):\n",
        "\n",
        "    # Sampling the classifiers\n",
        "    classifier = trial.suggest_categorical('classifier', ['RandomForest', 'SVC', 'LogisticRegression', 'XGBoost'])\n",
        "\n",
        "    # experimenting with dropping unnecessary columns and outlier handling\n",
        "    drop_and_winsorize = trial.suggest_int('drop_and_winsorize',0,1)\n",
        "\n",
        "    # experimenting with columns to drop\n",
        "    columns_to_drop = trial.suggest_categorical('columns_to_drop', ['useless', 'sparse'])\n",
        "\n",
        "    # experimenting with imputers\n",
        "    add_indicator = trial.suggest_int('add_indicator',0,1)\n",
        "    imputer = trial.suggest_categorical('imputer_type',['simple','knn'])\n",
        "\n",
        "    # experimenting with one hot encoding\n",
        "    keep_first = trial.suggest_int('keep_first',0,1)\n",
        "\n",
        "    pipeline_params = {\n",
        "    'imputer_type': imputer,\n",
        "    'drop_and_winsorize': drop_and_winsorize,\n",
        "    'classifier': classifier,\n",
        "    'columns_to_drop': columns_to_drop,\n",
        "    'add_indicator': add_indicator,\n",
        "    'keep_first': keep_first\n",
        "    }\n",
        "    pipeline = MLPipeline(**pipeline_params)\n",
        "\n",
        "   # constructing training pipeline\n",
        "    training_pipeline = pipeline.create_pipeline()\n",
        "\n",
        "    training_pipeline.fit(X_train,y_train)\n",
        "\n",
        "    scores = balanced_accuracy_score(y_val, training_pipeline.predict(X_val))\n",
        "    # scorer = make_scorer(balanced_accuracy_score)\n",
        "\n",
        "    # scores = cross_val_score(training_pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring=scorer)\n",
        "    # report_cross_validation_scores(trial, scores)\n",
        "\n",
        "    # Returning the cross-validated mean score\n",
        "    return scores.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpLhPFxghwvo"
      },
      "outputs": [],
      "source": [
        "study = optuna.create_study(direction=\"maximize\")\n",
        "# terminator = TerminatorCallback()\n",
        "study.optimize(objective, n_trials=100, n_jobs=-1, )#, #callbacks=[terminator])\n",
        "trial = study.best_trial\n",
        "\n",
        "print('f1_score: {}'.format(trial.value))\n",
        "print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzUCwacd3SHD"
      },
      "source": [
        "* Due to lack of resuurces we find the best pilpiline using 100000 snamples (10% of whole data)\n",
        "\n",
        "* Optuna giving a pipeline having Logistincregressin as eatimator due to such s sample size we dont have confidence in the model\n",
        "\n",
        "* We will perform cross validation to find the best hypermaters of multiple models of using transformation obtained from best piple using whole data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYZKgX_f4Zsf"
      },
      "source": [
        "## Model Selection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsJ1EH6ddlgo"
      },
      "source": [
        "### Approach\n",
        "\n",
        "- The initial phase of the study involved model selection using Optuna, where Logistic Regression was suggested as the initial model. However, the small sample size used in this phase cast doubts on the reliability of this selection.\n",
        "\n",
        "- To address this, a more detailed model selection process was undertaken. In this stage, various classifiers including XGBClassifier, RandomForestClassifier, and SVC were tested and evaluated. This process involved applying the transformations determined by Optuna's best pipeline to the entire dataset to pinpoint the most suitable model.\n",
        "\n",
        "- After identifying the best-suited model using the aforementioned strategy, Optuna will be employed again to fine-tune the hyperparameters of the selected model, with the aim of optimizing its performance further.\n",
        "\n",
        "- The overarching goal is to develop a robust machine learning pipeline, which is optimized for achieving a high balanced accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efr2hbGUqo3_"
      },
      "outputs": [],
      "source": [
        "# Best pipeline parameters obtained from cross validation using optuna\n",
        "best_pipeline_param = {\n",
        "    'classifier': 'LogisticRegression',\n",
        "    'drop_and_winsorize': 1,\n",
        "    'columns_to_drop': 'useless',\n",
        "    'add_indicator': 1,\n",
        "    'imputer_type': 'simple',\n",
        "    'keep_first': 1\n",
        "}\n",
        "\n",
        "# Creating the best MLPipeline with the best hyperparameters\n",
        "best_ml_pipeline = MLPipeline(**best_pipeline_param).create_pipeline()\n",
        "\n",
        "# Lists to store train and validation scores for each classifier\n",
        "train_score_list = []\n",
        "val_score_list = []\n",
        "\n",
        "# List of classifiers to evaluate\n",
        "clf_list = [XGBClassifier(), RandomForestClassifier(), LogisticRegression(), SVC()]\n",
        "\n",
        "for clf in clf_list:\n",
        "    # Updating the classifier in the pipeline\n",
        "    best_ml_pipeline.steps[-1] = ('model', clf)\n",
        "\n",
        "    # Fitting the pipeline to the training data\n",
        "    best_ml_pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluating the train score and storing it\n",
        "    train_score = balanced_accuracy_score(y_train, best_ml_pipeline.predict(X_train))\n",
        "    train_score_list.append(train_score)\n",
        "\n",
        "    # Evaluating the validation score and storing it\n",
        "    val_score = balanced_accuracy_score(y_val, best_ml_pipeline.predict(X_val))\n",
        "    val_score_list.append(val_score)\n",
        "\n",
        "    # Printing the train and validation scores for the current classifier\n",
        "    print(f\"\"\"\n",
        "    For model {clf}:\n",
        "        Train score = {train_score}\n",
        "        Validation score = {val_score}\n",
        "    \"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdDhDNOXUenz"
      },
      "source": [
        "* XGboost is gving the best performance using the best pipline obtained\n",
        "* Now we will find best hyper parameters XGboost to further improve the performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He0sUuTNVA3b"
      },
      "source": [
        "## Hyperparameter tuning of the best model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IGS5oy3W4Ix"
      },
      "source": [
        "### Hyperparameter Tuning Analysis:\n",
        "\n",
        "1. **Utilization of Optuna**: The Optuna framework, known for optimizing hyperparameters efficiently, was used during the tuning process of the XGBoost model.\n",
        "\n",
        "2. **Number of Trials**: The process comprised 23 trials, wherein various hyperparameter combinations were tested to ascertain the optimal configuration for the model.\n",
        "\n",
        "3. **Best Outcome**: Trial number 21 yielded the highest balanced accuracy score of approximately 0.8917. The parameters associated with this trial were:\n",
        "   - **n_estimators**: 250\n",
        "   - **max_depth**: 7\n",
        "   - **learning_rate**: ~0.0604\n",
        "   - **subsample**: ~0.767\n",
        "\n",
        "4. **Parameter Adjustments**: Across the trials, adjustments were made to parameters such as `n_estimators`, `max_depth`, `learning_rate`, and `subsample` to gauge their influence on the model's performance.\n",
        "\n",
        "5. **Performance Trends**: The trials exhibited a trend of gradual improvement in the balanced accuracy score, escalating from about 0.853 in the initial trials to approximately 0.892 in the final stages.\n",
        "\n",
        "### Conclusion:\n",
        "\n",
        "The section of the notebook dedicated to hyperparameter tuning delineates a structured approach undertaken to refine the XGBoost model for binary classification tasks. Utilizing the Optuna framework, 23 trials were conducted, systematically experimenting with different parameter combinations to enhance the balanced accuracy score, a critical metric in evaluating binary classification models. The efforts culminated in trial 21, which demonstrated a promising balanced accuracy score of around 0.8917, indicating a potential readiness for deployment in backorder prediction tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4C0jsCjGU9Rc"
      },
      "outputs": [],
      "source": [
        "# creating objective function to find the besst hyperparameter\n",
        "\n",
        "def objective_hp_tuning(trial):\n",
        "\n",
        "  # Best pipeline parameters obtained from cross validation using optuna\n",
        "  best_pipeline_param = {\n",
        "      'classifier': 'XGBoost',\n",
        "      'drop_and_winsorize': 1,\n",
        "      'columns_to_drop': 'useless',\n",
        "      'add_indicator': 1,\n",
        "      'imputer_type': 'simple',\n",
        "      'keep_first': 1\n",
        "  }\n",
        "\n",
        "  # Creating the best MLPipeline\n",
        "  best_ml_pipeline = MLPipeline(**best_pipeline_param).create_pipeline()\n",
        "\n",
        "  # Sampling hyperparameters for XGBoostClassifier\n",
        "  n_estimators = trial.suggest_int('xgb_n_estimators', 50, 300, step=50)\n",
        "  max_depth = trial.suggest_int('xgb_max_depth', 1, 10)\n",
        "  learning_rate = trial.suggest_loguniform('xgb_learning_rate', 0.01, 0.1)\n",
        "  subsample = trial.suggest_uniform('xgb_subsample', 0.6, 0.9)\n",
        "\n",
        "  # putting the hyperparameter in kwarg args\n",
        "  hyperparams = {\n",
        "    'model__n_estimators': n_estimators,\n",
        "    'model__max_depth': max_depth,\n",
        "    'model__learning_rate': learning_rate,\n",
        "    'model__subsample': subsample\n",
        "  }\n",
        "\n",
        "  # passing the hyperparameters to the estimator\n",
        "  best_ml_pipeline.set_params(**hyperparams)\n",
        "\n",
        "  # Fitting the pipeline to the training data\n",
        "  best_ml_pipeline.fit(X_train, y_train)\n",
        "\n",
        "  # Evaluating the validation score\n",
        "  val_score = balanced_accuracy_score(y_val, best_ml_pipeline.predict(X_val))\n",
        "\n",
        "  return val_score\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cQ7mOdq2lnr",
        "outputId": "0f7e1edd-4183-4984-a691-5cf22389777c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-08-05 06:24:03,589] A new study created in memory with name: no-name-c22358fd-fc26-4f9a-95e9-11158f8fa3d1\n",
            "[I 2023-08-05 06:27:30,060] Trial 1 finished with value: 0.853263642375435 and parameters: {'xgb_n_estimators': 50, 'xgb_max_depth': 2, 'xgb_learning_rate': 0.07026723290192255, 'xgb_subsample': 0.8594545021190145}. Best is trial 1 with value: 0.853263642375435.\n",
            "[I 2023-08-05 06:38:46,802] Trial 2 finished with value: 0.8591545831490622 and parameters: {'xgb_n_estimators': 200, 'xgb_max_depth': 2, 'xgb_learning_rate': 0.025178702998072428, 'xgb_subsample': 0.7432464552214986}. Best is trial 2 with value: 0.8591545831490622.\n",
            "[I 2023-08-05 06:58:14,994] Trial 0 finished with value: 0.8886249601343184 and parameters: {'xgb_n_estimators': 100, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.02251088885825726, 'xgb_subsample': 0.6344097333650928}. Best is trial 0 with value: 0.8886249601343184.\n",
            "[I 2023-08-05 07:21:20,897] Trial 3 finished with value: 0.883111317111936 and parameters: {'xgb_n_estimators': 250, 'xgb_max_depth': 6, 'xgb_learning_rate': 0.014346169404563939, 'xgb_subsample': 0.8098765597235349}. Best is trial 0 with value: 0.8886249601343184.\n",
            "[I 2023-08-05 07:44:49,341] Trial 5 finished with value: 0.8637937924889226 and parameters: {'xgb_n_estimators': 300, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.010338859685821263, 'xgb_subsample': 0.7631254871328619}. Best is trial 0 with value: 0.8886249601343184.\n",
            "[I 2023-08-05 07:56:03,839] Trial 6 finished with value: 0.8767436950534194 and parameters: {'xgb_n_estimators': 150, 'xgb_max_depth': 3, 'xgb_learning_rate': 0.08444462154904732, 'xgb_subsample': 0.6832973972019535}. Best is trial 0 with value: 0.8886249601343184.\n",
            "[I 2023-08-05 08:15:58,655] Trial 7 finished with value: 0.8805043413567268 and parameters: {'xgb_n_estimators': 200, 'xgb_max_depth': 4, 'xgb_learning_rate': 0.04450388078073262, 'xgb_subsample': 0.7238689724529207}. Best is trial 0 with value: 0.8886249601343184.\n",
            "[I 2023-08-05 08:35:11,066] Trial 4 finished with value: 0.8863849610324642 and parameters: {'xgb_n_estimators': 300, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.011913670074857309, 'xgb_subsample': 0.8602080836968915}. Best is trial 0 with value: 0.8886249601343184.\n",
            "[I 2023-08-05 08:47:22,630] Trial 9 finished with value: 0.8607353579013539 and parameters: {'xgb_n_estimators': 250, 'xgb_max_depth': 2, 'xgb_learning_rate': 0.022846320416293375, 'xgb_subsample': 0.8144265964442127}. Best is trial 0 with value: 0.8886249601343184.\n",
            "[I 2023-08-05 08:55:59,337] Trial 10 finished with value: 0.8523885486758895 and parameters: {'xgb_n_estimators': 300, 'xgb_max_depth': 1, 'xgb_learning_rate': 0.04275148850179188, 'xgb_subsample': 0.6548672900078144}. Best is trial 0 with value: 0.8886249601343184.\n",
            "[I 2023-08-05 08:57:35,446] Trial 8 finished with value: 0.890104798679255 and parameters: {'xgb_n_estimators': 150, 'xgb_max_depth': 9, 'xgb_learning_rate': 0.025440736443630884, 'xgb_subsample': 0.6955603998421735}. Best is trial 8 with value: 0.890104798679255.\n",
            "[I 2023-08-05 09:11:52,266] Trial 11 finished with value: 0.8825142657125025 and parameters: {'xgb_n_estimators': 50, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.015973864019587822, 'xgb_subsample': 0.6030238685815291}. Best is trial 8 with value: 0.890104798679255.\n",
            "[I 2023-08-05 09:28:18,981] Trial 12 finished with value: 0.8878227767509539 and parameters: {'xgb_n_estimators': 100, 'xgb_max_depth': 10, 'xgb_learning_rate': 0.019925833248788626, 'xgb_subsample': 0.6032756396528051}. Best is trial 8 with value: 0.890104798679255.\n",
            "[I 2023-08-05 09:34:48,648] Trial 13 finished with value: 0.8870165744520506 and parameters: {'xgb_n_estimators': 100, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.020092237305979314, 'xgb_subsample': 0.6413652480930749}. Best is trial 8 with value: 0.890104798679255.\n",
            "[I 2023-08-05 09:51:25,294] Trial 14 finished with value: 0.8892599938797471 and parameters: {'xgb_n_estimators': 100, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.030975992436102245, 'xgb_subsample': 0.6650067489659944}. Best is trial 8 with value: 0.890104798679255.\n",
            "[I 2023-08-05 10:08:29,940] Trial 15 finished with value: 0.8897521633462931 and parameters: {'xgb_n_estimators': 150, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.03122166578944892, 'xgb_subsample': 0.6963416998179347}. Best is trial 8 with value: 0.890104798679255.\n",
            "[I 2023-08-05 10:25:10,134] Trial 16 finished with value: 0.8905684599847623 and parameters: {'xgb_n_estimators': 150, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.03373935195210825, 'xgb_subsample': 0.6885112167273493}. Best is trial 16 with value: 0.8905684599847623.\n",
            "[I 2023-08-05 10:42:23,954] Trial 17 finished with value: 0.8906186310807793 and parameters: {'xgb_n_estimators': 150, 'xgb_max_depth': 8, 'xgb_learning_rate': 0.03633390572798139, 'xgb_subsample': 0.71089586121332}. Best is trial 17 with value: 0.8906186310807793.\n",
            "[I 2023-08-05 10:48:34,597] Trial 18 finished with value: 0.8866143574640175 and parameters: {'xgb_n_estimators': 150, 'xgb_max_depth': 6, 'xgb_learning_rate': 0.043414977447681906, 'xgb_subsample': 0.7054828303914853}. Best is trial 17 with value: 0.8906186310807793.\n",
            "[I 2023-08-05 11:13:10,812] Trial 19 finished with value: 0.8879554702328532 and parameters: {'xgb_n_estimators': 200, 'xgb_max_depth': 6, 'xgb_learning_rate': 0.043632134408222605, 'xgb_subsample': 0.7242284390369765}. Best is trial 17 with value: 0.8906186310807793.\n",
            "[I 2023-08-05 11:25:31,320] Trial 20 finished with value: 0.8912268498375535 and parameters: {'xgb_n_estimators': 200, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.056520314852108657, 'xgb_subsample': 0.7355810166719106}. Best is trial 20 with value: 0.8912268498375535.\n",
            "[I 2023-08-05 11:58:21,830] Trial 21 finished with value: 0.8917106612667429 and parameters: {'xgb_n_estimators': 250, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.06038443608301605, 'xgb_subsample': 0.7669681486745225}. Best is trial 21 with value: 0.8917106612667429.\n",
            "[I 2023-08-05 12:10:08,688] Trial 22 finished with value: 0.891104201684055 and parameters: {'xgb_n_estimators': 250, 'xgb_max_depth': 7, 'xgb_learning_rate': 0.05677384921705064, 'xgb_subsample': 0.7620601312669847}. Best is trial 21 with value: 0.8917106612667429.\n"
          ]
        }
      ],
      "source": [
        "# Create an Optuna study object for hyperparameter tuning with \"maximize\" direction to maximize the objective function\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "\n",
        "# Perform hyperparameter tuning using the objective_hp_tuning function as the objective function\n",
        "# n_trials: Number of trials (iterations) for hyperparameter tuning\n",
        "# n_jobs: Number of parallel jobs to run in parallel. Use -1 to use all available CPU cores.\n",
        "study.optimize(objective_hp_tuning, n_trials=100, n_jobs=-1)\n",
        "\n",
        "# Get the best trial from the study (the trial with the highest value of the objective function)\n",
        "trial = study.best_trial\n",
        "\n",
        "# Print the balanced score obtained from the best trial\n",
        "print('balanced score: {}'.format(trial.value))\n",
        "\n",
        "# Print the best hyperparameters found from the hyperparameter tuning process\n",
        "print(\"Best hyperparameters: {}\".format(trial.params))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJD2PzUlaUCw"
      },
      "source": [
        "# Model training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQBlgx5lkBc7",
        "outputId": "5fbd04c8-f461-462c-8bfe-72adde156d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1687860, 22)\n",
            "(1687860,)\n"
          ]
        }
      ],
      "source": [
        "# Load data\n",
        "X_train = pd.read_csv(r\"/content/dataset/Kaggle_Training_Dataset_v2.csv\")\n",
        "\n",
        "# Drop rows with NaNs\n",
        "X_train.dropna(subset=X_train.columns.drop('lead_time'), inplace=True)\n",
        "\n",
        "# Encode target column\n",
        "y_train = LabelEncoder().fit_transform(X_train['went_on_backorder'])\n",
        "\n",
        "# Remove target column from features\n",
        "X_train = X_train.drop('went_on_backorder', axis=1)\n",
        "\n",
        "# Print shapes\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fJWdRk1crh7O",
        "outputId": "622a452b-49ee-405a-9aff-f24f4354034d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training begins\n",
            "training finished\n"
          ]
        }
      ],
      "source": [
        "# parmas for best ml pipeline\n",
        "best_pipeline_param = {\n",
        "    'classifier': 'XGBoost',\n",
        "    'drop_and_winsorize': 1,\n",
        "    'columns_to_drop': 'useless',\n",
        "    'add_indicator': 1,\n",
        "    'imputer_type': 'simple',\n",
        "    'keep_first': 1\n",
        "}\n",
        "# creating the comple pipeline\n",
        "best_ml_pipeline = CustomMLPipeline(**best_pipeline_param).create_pipeline()\n",
        "\n",
        "# hyperparameters\n",
        "model_hyperparams = {\n",
        "    'model__n_estimators': 250,\n",
        "    'model__max_depth': 7,\n",
        "    'model__learning_rate': 0.06038443608301605,\n",
        "    'model__subsample': 0.7669681486745225\n",
        "}\n",
        "# passing the hyperparameters to estimator of pipeline\n",
        "best_ml_pipeline.set_params(**model_hyperparams)\n",
        "\n",
        "# training begins\n",
        "print('training begins')\n",
        "best_ml_pipeline.fit(X_train,y_train)\n",
        "print('training finished')\n",
        "\n",
        "# Save the pipeline as a pickle file\n",
        "import pickle\n",
        "with open('ml_pipeline.pkl', 'wb') as file:\n",
        "    pickle.dump(best_ml_pipeline, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lrgAzeXAOM82"
      },
      "outputs": [],
      "source": [
        "# loading the trained pipline pickle\n",
        "import pickle\n",
        "\n",
        "# Load the trained pipeline from the pickle file\n",
        "with open('ml_pipeline.pkl', 'rb') as file:\n",
        "    best_ml_pipeline = pickle.load(file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "paDgs-ddgrJ7"
      },
      "source": [
        "# Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujoGgAIZcQZY"
      },
      "source": [
        "### Model Evaluation Results\n",
        "- **Balanced Accuracy:** The model achieved an impressive 84% balanced accuracy score on the test data, showcasing its ability to predict both backorders and non-backorders effectively.\n",
        "\n",
        "- **ROC-AUC Score:** With a score of 0.93, the model excels at differentiating products at risk of backorder, enabling proactive decision-making.\n",
        "\n",
        "- **Precision and Recall:** While the model exhibits high recall (0.77), indicating it captures most backorders, the precision (0.09) suggests some false positives. Careful consideration is needed to balance operational costs.\n",
        "\n",
        "### Business Impact\n",
        "The implementation of the model has had a notable impact on business:\n",
        "\n",
        "- **Operational Efficiency:** The model's deployment has optimized inventory management and supply chain operations, reducing the likelihood of delayed deliveries and enhancing operational efficiency.\n",
        "\n",
        "- **Cost Savings:** By accurately predicting backorders, the business has realized significant cost savings through better resource allocation and reduced storage expenses.\n",
        "\n",
        "- **Customer Satisfaction:** Timely deliveries and product availability have improved customer satisfaction, potentially boosting customer loyalty.\n",
        "\n",
        "- **Issue of Low Precision:** However, the low precision presents a challenge. False positive predictions can lead to excess costs and resource allocation. Addressing this issue is critical to optimize business outcomes.\n",
        "\n",
        "### Further Steps\n",
        "To enhance the model's precision and overall performance, further steps involve implementing cross-validation techniques using the F1 score as the primary evaluation metric. This will ensure that the model not only captures backorders effectively but also minimizes false positives, aligning it more closely with business objectives."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rfBmdJ04hFO3",
        "outputId": "9d69b782-800b-4674-ec94-36d65b7a5531"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(242075, 22)\n",
            "(242075,)\n"
          ]
        }
      ],
      "source": [
        "# loading test data\n",
        "X_test = pd.read_csv(r\"/content/dataset/Kaggle_Test_Dataset_v2.csv\")\n",
        "\n",
        "# Drop rows with NaNs\n",
        "X_test.dropna(subset=X_test.columns.drop('lead_time'), inplace=True)\n",
        "\n",
        "# Encode target column\n",
        "y_test = LabelEncoder().fit_transform(X_test['went_on_backorder'])\n",
        "\n",
        "# Remove target column from features\n",
        "X_test = X_test.drop('went_on_backorder', axis=1)\n",
        "\n",
        "# Print shapes\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4OybmWH5gjw"
      },
      "outputs": [],
      "source": [
        "# Predict training labels\n",
        "y_train_pred = best_ml_pipeline.predict(X_train)\n",
        "\n",
        "# Predict training probabilities\n",
        "y_train_pred_proba = best_ml_pipeline.predict_proba(X_train)[:, 1]\n",
        "\n",
        "# Predict test labels\n",
        "y_test_pred = best_ml_pipeline.predict(X_test)\n",
        "\n",
        "# Predict test probabilities\n",
        "y_test_pred_proba = best_ml_pipeline.predict_proba(X_test)[:, 1]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 965
        },
        "id": "zpxmWQj8LTJH",
        "outputId": "3015b093-f9b1-4fab-a587-572ed1f82df5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model evaluation report:\n",
            "\n",
            "+-------------------------+-------------+------------+\n",
            "|         Metric          | Train Score | Test Score |\n",
            "+-------------------------+-------------+------------+\n",
            "| balanced_accuracy_score |    0.93     |    0.84    |\n",
            "|      roc_auc_score      |    0.97     |    0.93    |\n",
            "|     precision_score     |    0.06     |    0.09    |\n",
            "|      recall_score       |    0.95     |    0.77    |\n",
            "|        f1_score         |    0.11     |    0.16    |\n",
            "+-------------------------+-------------+------------+\n",
            "\n",
            "Validation Confusion matrix:\n",
            "\n",
            "+-----------------+--------------------+--------------------+\n",
            "|                 | Predicted Negative | Predicted Positive |\n",
            "+-----------------+--------------------+--------------------+\n",
            "| Actual Negative |    TNR = 91.10     |     FPR = 8.90     |\n",
            "| Actual Positive |    FNR = 23.40     |    TPR = 76.60     |\n",
            "+-----------------+--------------------+--------------------+\n",
            "\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArMAAAIjCAYAAAAQgZNYAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAACiMklEQVR4nOzdd3hUZd7G8e9MeiEFQqiBUKRLl46A9CYtiKJrV/QVV0VXZV3rurK7ui666mIDFhtIKCJVpKggUkVAeq8hQCA9mczMef8YMyEmQAKZOSn357rmOmfOnHJPCMkvzzzneSyGYRiIiIiIiJRBVrMDiIiIiIhcLRWzIiIiIlJmqZgVERERkTJLxayIiIiIlFkqZkVERESkzFIxKyIiIiJllopZERERESmzVMyKiIiISJmlYlZEREREyiwVsyIics1iY2O5++67zY4hIhWQilkRuSrTp0/HYrG4H76+vtSqVYu7776bEydOFHqMYRh88skn3HjjjURERBAcHMz111/PK6+8Qnp6+iWvNW/ePAYOHEhUVBT+/v7UrFmTW265hZUrVxYpa1ZWFv/+97/p2LEj4eHhBAYG0qhRI8aPH8/evXuv6v2XBatXr873b3S5R2n1v//9jxYtWhAcHExMTAx33nknJ0+eLNKxRX3vq1evvuacGRkZvPTSSyVyLhEpHl+zA4hI2fbKK69Qr149srKy+Omnn5g+fTpr1qxhx44dBAYGuvdzOByMHTuWL7/8ku7du/PSSy8RHBzMDz/8wMsvv8zs2bP59ttvqVatmvsYwzC49957mT59Om3atGHChAlUr16dU6dOMW/ePHr37s3atWvp0qXLJfOdPXuWAQMGsHnzZoYMGcLYsWMJDQ1lz549zJw5kw8++ACbzebRr5FZmjZtyieffJJv28SJEwkNDeW5554r0Wvt2bMHq7Vk20fmzZvH3XffTY8ePRg/fjyJiYnEx8ezd+9eatasecXjf//eZ8yYwfLlywtsb9q06TVnzcjI4OWXXwagZ8+e13w+ESkGQ0TkKkybNs0AjI0bN+bb/swzzxiAMWvWrHzbX3vtNQMwnnrqqQLnWrBggWG1Wo0BAwbk2/76668bgPH4448bTqezwHEzZsww1q9ff9mcgwcPNqxWqxEfH1/gtaysLOPJJ5+87PFFlZOTY2RnZ5fIuTypefPmRo8ePS67j8PhMDIzM70T6DJuueUWo3LlygWyXO3X+ZFHHjE89WvvzJkzBmC8+OKLHjm/iFyauhmISInq3r07AAcOHHBvy8zM5PXXX6dRo0ZMmjSpwDFDhw7lrrvuYunSpfz000/uYyZNmkSTJk144403Cv0o/A9/+AMdOnS4ZJb169ezaNEi7rvvPkaNGlXg9YCAAN544w338549exbaqnb33XcTGxvrfn748GEsFgtvvPEGkydPpkGDBgQEBPDzzz/j6+vrbqG72J49e7BYLLzzzjvubRcuXODxxx8nJiaGgIAAGjZsyD/+8Q+cTucl35MnWCwWxo8fz2effUbz5s0JCAhg6dKlALzxxht06dKFKlWqEBQURLt27YiPjy9wjt/3mc3thrJ27VomTJhA1apVCQkJYcSIEZw5c6ZIuaxWK3a7HR8fn3zb/f39r/7N/o7T6WTy5Mk0b96cwMBAqlWrxrhx4zh//ny+/TZt2kT//v2JiooiKCiIevXqce+99wKu74eqVasC8PLLL7u7L7z00kslllNELk3dDESkRB0+fBiAyMhI97Y1a9Zw/vx5HnvsMXx9C/+xc+eddzJt2jQWLlxIp06dWLNmDUlJSTz++OMFipmiWrBgAeAqej1h2rRpZGVl8eCDDxIQEECNGjXo0aMHX375JS+++GK+fWfNmoWPjw+jR48GXB9L9+jRgxMnTjBu3Djq1KnDjz/+yMSJEzl16hSTJ0/2SOZLWblyJV9++SXjx48nKirKXby/9dZb3Hzzzdx+++3YbDZmzpzJ6NGjWbhwIYMHD77ieR999FEiIyN58cUXOXz4MJMnT2b8+PHMmjXrisfec889zJw5kxdeeKHQP4JKwrhx45g+fTr33HMPf/zjHzl06BDvvPMOP//8M2vXrsXPz4/ExET69etH1apVefbZZ4mIiODw4cPMnTsXgKpVq/Lf//6Xhx9+mBEjRjBy5EgAWrZs6ZHMIpKfilkRuSbJycmcPXuWrKws1q9fz8svv0xAQABDhgxx77Nz504AWrVqdcnz5L62a9eufMvrr7/+qrOVxDku5/jx4+zfv9/dKgcwZswYxo0bx44dO2jRooV7+6xZs+jRo4e7T/Cbb77JgQMH+Pnnn7nuuusAV2FVs2ZNXn/9dZ588kliYmI8krswe/bsYfv27TRr1izf9r179xIUFOR+Pn78eNq2bcubb75ZpGK2SpUqfPPNN+6WdafTydtvv01ycjLh4eGXPfbAgQMEBATw97//nRo1avDHP/7xKt7Zpa1Zs4aPPvqIzz77jLFjx7q39+rViwEDBjB79mzGjh3Ljz/+yPnz5/nmm29o3769e79XX30VgJCQEOLi4nj44Ydp2bIld9xxR4nmFJHLUzcDEbkmffr0oWrVqsTExBAXF0dISAgLFiygdu3a7n1SU1MBqFSp0iXPk/taSkpKvuXljrmSkjjH5YwaNSpfIQswcuRIfH1987U87tixg507dzJmzBj3ttmzZ9O9e3ciIyM5e/as+9GnTx8cDgfff/+9RzJfSo8ePQoUskC+Qvb8+fMkJyfTvXt3tmzZUqTzPvjgg/m6iHTv3h2Hw8GRI0cue9xXX33FI488Qnx8PM899xyPP/4406ZNy7dP48aNr6nVffbs2YSHh9O3b998/wbt2rUjNDSUVatWARAREQHAwoULycnJuerriYhnqGVWRK7Ju+++S6NGjUhOTmbq1Kl8//33BAQE5Nsnt5jMLWoL8/uCNyws7IrHXMnF58gtSEpSvXr1CmyLioqid+/efPnll/z1r38FXK2yvr6+7o+fAfbt28e2bdsKFMO5EhMTL3nd5ORkMjMz3c/9/f2pXLny1b4NoPD3Aq4C7tVXX2Xr1q1kZ2e7txd1OK86derke57b/eT3fVJ/75lnnmHgwIEMGTKEIUOGcPr0aR544AEqVapEXFwcGRkZHDp0iEcffbRIOQqzb98+kpOTiY6OLvT13H+DHj16MGrUKF5++WX+/e9/07NnT4YPH87YsWMLfK+LiPepmBWRa9KhQwf3R6/Dhw+nW7dujB07lj179hAaGgrkDX20bds2hg8fXuh5tm3bBuBuHWzSpAkA27dvv+QxV3LxOXJvTLsci8WCYRgFtjscjkL3v7jV8mK33nor99xzD1u3bqV169Z8+eWX9O7dm6ioKPc+TqeTvn378vTTTxd6jkaNGl0y52OPPcb//vc/9/MePXpc8/imhb2XH374gZtvvpkbb7yR9957jxo1auDn58e0adP4/PPPi3TeS/V3LuzrnCspKYk9e/Zw++23u7dNmTKFM2fOMHbsWEJCQjh48CBWq5W4uLgi5SiM0+kkOjqazz77rNDXc//QsFgsxMfH89NPP/H111+zbNky7r33Xv71r3/x008/ub/PRcQcKmZFpMT4+PgwadIkevXqxTvvvMOzzz4LQLdu3YiIiODzzz/nueeeK7TAmTFjBoC7r223bt2IjIzkiy++4M9//vNV3QQ2dOhQJk2axKefflqkYjYyMpKDBw8W2H6lj8R/b/jw4YwbN87d1WDv3r1MnDgx3z4NGjQgLS2NPn36FOvcAE8//XS+fpkX32xXkubMmUNgYCDLli3L1wL5+4/7S1puq++xY8fc23x8fJg5cyb9+vVj1KhRhIWF8fDDD1O9evWrvk6DBg349ttv6dq16yX/MLlYp06d6NSpE3/729/4/PPPuf3225k5cyb3339/qZ54QqS8U59ZESlRPXv2pEOHDkyePJmsrCwAgoODeeqpp9izZ0+hg/UvWrSI6dOn079/fzp16uQ+5plnnmHXrl0888wzhbbkffrpp2zYsOGSWTp37syAAQP46KOPmD9/foHXbTYbTz31lPt5gwYN2L17d76ho3755RfWrl1b5PcPrj6W/fv358svv2TmzJn4+/sXaF2+5ZZbWLduHcuWLStw/IULF7Db7Zc8f7NmzejTp4/70a5du2LlKyofHx8sFku+lunDhw8X+rUsSZGRkbRt25bPP/+c3bt3u7cHBgbyySef4HQ6OX369FW32Oe65ZZbcDgc7u4gF7Pb7Vy4cAFwdYn4/fdf69atAdxdL4KDgwHcx4iI96hlVkRK3J/+9CdGjx7N9OnTeeihhwB49tln+fnnn/nHP/7BunXrGDVqFEFBQaxZs4ZPP/2Upk2b5vvoPPc8v/76K//6179YtWoVcXFxVK9enYSEBObPn8+GDRv48ccfL5tlxowZ9OvXj5EjRzJ06FB69+5NSEgI+/btY+bMmZw6dco91uy9997Lm2++Sf/+/bnvvvtITExkypQpNG/e3H0zWVGNGTOGO+64g/fee4/+/fsX6LP7pz/9iQULFjBkyBDuvvtu2rVrR3p6Otu3byc+Pp7Dhw/n65ZghsGDB/Pmm28yYMAAxo4dS2JiIu+++y4NGzZ0dwvxlP/85z/06dOHDh06MG7cOJo0acLhw4eZOnUq1apVw2q1MnbsWNavX5/vZsPi6NGjB+PGjWPSpEls3bqVfv364efnx759+5g9ezZvvfUWcXFx/O9//+O9995jxIgRNGjQgNTUVD788EPCwsIYNGgQ4Oqm0axZM2bNmkWjRo2oXLkyLVq0yDeihYh4iLlzNohIWXWpGcAMwzWDVIMGDYwGDRoYdrs93/Zp06YZXbt2NcLCwozAwECjefPmxssvv2ykpaVd8lrx8fFGv379jMqVKxu+vr5GjRo1jDFjxhirV68uUtaMjAzjjTfeMG644QYjNDTU8Pf3N6677jrj0UcfNfbv359v308//dSoX7++4e/vb7Ru3dpYtmyZcddddxl169Z173Po0CEDMF5//fVLXjMlJcUICgoyAOPTTz8tdJ/U1FRj4sSJRsOGDQ1/f38jKirK6NKli/HGG28YNputSO+tuAqbAQwwHnnkkUL3//jjj43rrrvOCAgIMJo0aWJMmzbNePHFFwvMpFW3bl3jrrvucj+/1PfHqlWrDMBYtWrVFbNu27bNGDlypFG5cmX3v9nEiRONpKQkY+vWrUZQUJDRqlUrIyUlpUjv/VIzgH3wwQdGu3btjKCgIKNSpUrG9ddfbzz99NPGyZMnDcMwjC1bthi33XabUadOHSMgIMCIjo42hgwZYmzatCnfeX788UejXbt2hr+/v2YDE/Eii2Fcphe+iIiIiEgppj6zIiIiIlJmqZgVERERkTJLxayIiIiIlFkqZkVERESkzFIxKyIiIiJllopZERERESmzKtykCU6nk5MnT1KpUiVNPygiIiJSChmGQWpqKjVr1sRqvXzba4UrZk+ePElMTIzZMURERETkCo4dO3bFWf4qXDFbqVIlwPXFCQsLMzmNiIiIiPxeSkoKMTEx7rrtcipcMZvbtSAsLEzFrIiIiEgpVpQuoboBTERERETKLBWzIiIiIlJmqZgVERERkTJLxayIiIiIlFkqZkVERESkzFIxKyIiIiJllopZERERESmzVMyKiIiISJmlYlZEREREyiwVsyIiIiJSZqmYFREREZEyS8WsiIiIiJRZKmZFREREpMxSMSsiIiIiZZapxez333/P0KFDqVmzJhaLhfnz51/xmNWrV9O2bVsCAgJo2LAh06dP93hOERERESmdTC1m09PTadWqFe+++26R9j906BCDBw+mV69ebN26lccff5z777+fZcuWeTipiIiIiJRGvmZefODAgQwcOLDI+0+ZMoV69erxr3/9C4CmTZuyZs0a/v3vf9O/f39PxRQRKdOchpMcRw42h40cZ84l189lnMPutHslk4HhnesY3rkO6D1d03Uq6HtyOgtuS0kKIPWCf/GudQ3v6eThShTpy284wWJlwh0tqF459Kqv5wmmFrPFtW7dOvr06ZNvW//+/Xn88ccveUx2djbZ2dnu5ykpKZ6KJ1Lh5ThyyLRn4jScGIbhWmLkW7c5bJzNOIvTKOSn+G+y7dmk2lLJzMks0nVzz3vxI9uejc1hK6m3hoFBSnYKZzPOkmXP4kLWBZKzk7E77RiG4X6fv1/mHgsUeF6S2y5VoNoctst+rUUqnKwwcPpAViQktIKUGDjVFkJOF77/vsEQeeDK5z3bFNKqQWhC0XIkNSp6ZlMZtG37M506/cTUqfcyoOtpFbPXIiEhgWrVquXbVq1aNVJSUsjMzCQoKKjAMZMmTeLll1/2VkSREpVlzyIjJwObw0aaLY0sexZOw4nD6cBpOF3rhmvd7rSTbkt3FzMZORnsT9rP3qS9ZORkuPcv7OFwOsi05y8cDcMgy55Fqi2V1OzUy7bYGbj29Varnlw7fx9//Kx+rqWPH35WP8IDwwnyLfhz1BssFov3r4n3rwnl7706soLJSY0o/Lq/vdfsc9Vw2gILHptRiawztfEJyCj0eKfDl5wLUThzAgp9Pe1QM3BayU6qjsU394/XS79Xe2rkpd/I5ZxpXvR9k8Ku7hq/4xt6oUTOcyWGwxdHZiiVGm4r8Jqfbw59O26lab3jAHTt/zVhId28kqs4ylQxezUmTpzIhAkT3M9TUlKIiYkxMZGUV2czzpKSnYLD6cBhOMjIySAlO4UsexYOpwO7047DcC3tTjtZ9ix362GaLY3DFw6z9fRW9iftd7fspWSXv08S/Kx+VA2piq/10j9+/Kx+VAqoRJBvUJF/8fv7+BPgE4C/j7/74efjh9VScrcGhPqFEh0STaBvIBGBEUQERuBr9cVisWDBUugSKLAO+Quakth2cVFa2Hpu8ern44ePxceUgkqK7sIFOHw4/8fQTifs3g3nz8O+fWAYsHUrJCd7JsP27a6ln9+l98nJ8cy1ve2GGyAkBNq1K/iaYbi+xp06Xfk8mZlw3XUQWsSGy7AwqFkz/zZfX4iIAIgo2klKTMt8zxISEpg9ezZJSUlYLBZuuukmXniha6n82VGmitnq1atz+nT+jwFOnz5NWFhYoa2yAAEBAQQEFP4XnUhR2Rw29p7by47EHew6s4vT6ac5kXqC1YdX06xqM5Iyk9iftN9j17dgwd/Hn2xHNtVCqmG1WPGx+mC1WF3rFtd6iH8IAT4B7uIlNjyWplWbEhEY4d73Ug9/H/8CBWaATwCVAipRyb8Sfj6X+Y0GBPkGEewXTJBfEFaL1V3AXbwuUl7s3g0bNkBa2qX3SUmBNWvA9rveLobhei0pCS7qBeeWnQ2JiSWb91oUtWANCbn0a06nq9ArrFhMSIDataFhw8KP9fOD6tUvff6kJGjbFurVg/DwK+eMinI9AHx8QD+a8jMMg02bNrFs2TIcDgdhYWHExcWV6obAMlXMdu7cmcWLF+fbtnz5cjp37mxSIinLMnIySExPJMeRg91p50LWBXad3cWJlBOk2lI5n3meC9kX2HVmF3vO7bnkR+gbTmxwrwf7BeNr9cXH4kOwXzBhAWEE+ga6tll93K/5Wn0J8A0gwCeAAN8AgnyDqB1Wm+ujr6dZ1Wb4+7g6/1cJrkJ4QDg+Vh+vfE1EKgKnE1asgNRU1/N16yDyok+f588Hq7XgzTkbN3otIgBVq4L/7+4DcjpdLXk5OVCjBnTr5mpV9PHQj4jAQIiNvfw+Vqsri4rC8iEpKYmlS5fidDpp1KgRw4YNIzg42OxYl2VqMZuWlsb+/XmtWYcOHWLr1q1UrlyZOnXqMHHiRE6cOMGMGTMAeOihh3jnnXd4+umnuffee1m5ciVffvklixYtMustSBmRmJ7IqkOrWLhvIQfPH+RM+hnXx/nFuAM0LCCMFtEtaBbVjCrBVYgJiyEyKJKMnAxqVqpJ+5rtiQ6J9uC7ECl/nE7XR+VZWQVfS02F/fshKAjsdvjhB1fR9HuLFkGDBle+Vno6rFx5zZHzGTXq8q+3bAn16xfcXqkSVK7sem+/Z7W6jnF91CziXVWqVKF///44HA46depUJj5VM7WY3bRpE7169XI/z+3betdddzF9+nROnTrF0aNH3a/Xq1ePRYsW8cQTT/DWW29Ru3ZtPvroIw3LJfkkpCWw6eQm4nfGc/D8QdYeW3vJu7lzW0b9rH74Wn2pHlqddjXauftDhgWE0TiqMc2rNqd2WO0y8Z9apKSkpcHZs+BwuD4qz8pyfQSelVX4+nffQXDwpVvoDAOOHHHt73TC0aNw7FjJZN258+qO69rVleXYMcj9VWIYrsfIkYUf07ixq19jbKxaI6XsMwyDDRs2ULduXapXrw5Ahw4dTE5VPBbDm4O7lQIpKSmEh4eTnJxMWFjJ3HEo5sm2Z5OUmcS209v4z4b/kJKdwg9Hf7jk/h1rdSSuWRxtqrehQeUG1A2vqwJVKpzt2+Hdd2HzZlc/Q3DdVPTtt651Pz9XMWf38uAUhbWunjzp6k/ZqJGroE5Ph9/3LDMM1w1TV+pxZrW63m9QEFx//eX7eIpUBJmZmSxYsIDdu3dTuXJlxo0bh//v+7aYpDj1WpnqMyuSbc/mix1f8Pb6tzmdfpqTqScL3S8iMILe9XpTP7I+N9S8gZvq3USV4CpeTity9dLSXDcJrV8Pp065Wg8dDleB6XDkX//9tq++KlgY2u2wY4erD+aJE3nbN20qeO2Lb/gJCHC1QgYEuB6BgXmPi5/nrh85AoMGXbrFsmpVqPLbf8XgYIiJcRWYRb37W0RKxvHjx4mPjyc5ORkfHx86duyI3+WGrijFVMxKqfbzqZ95b+N7nM08y0/HfyIhrfDBqGtWqkn3Ot2pGlyV8R3G0ziqsZeTiuR9jJ5RyJCZZ8+6Hr+Xk+O6K95igdOnYfbswu9wL66DB6+8T6tW0KSJ6yYicLV8VqkC3bu7it6ICBWZIuWNYRisW7eOFStW4HQ6iYyMJC4ujpq/HyOsDFExK6WOYRh8d+Q7nvrmKTaf2lzg9bCAMHrG9qRR5Ub8seMfqRVWq0THEhUprqwsmDsXHnoo7w75kuLj42pxjYtzrfv65l9ealtaGhTW7S13WKJGjVxjXIpIxWGz2ZgzZw579+4FoHnz5gwdOrTMD2GqYlZKhb3n9jL5p8lsObWFzac25xsGq33N9jSs3JDRzUYTExZD+5rt1c9VvMowYNUqV1/TRYtcH9c3aOD66P/HHws/psrverVYrVCtWuHjYObeRHXbba4W0agouOkmqFu35N+LiFRcfn5+2O12fHx8GDBgAO3atSsXv09VzIqpfjz2I/9Y+w8W7FlQ6OtLbl/CgIYDvJxKKhqHA7ZtcxWt4Pq4/cgRWLIE9u51jUP6e6dOFX6uAQPg669draQiImYzDAOHw4Gvr2u2whEjRpCWluYeuaA80I9b8Tqbw8aXv37JR1s+4rsj37m3hweEM6LpCP6v/f/RqEojwgOLMJWLSDE4na7W1aVLXX1BFyxwFbEJhXfFLpSfH9x4o2vGoQ4dXAVwlSrQo4fnBq4XEbka6enpzJs3j/DwcIYOHQpAaGgooeWsM7yKWfE4wzA4dOEQKw+tZMOJDczcMZNUm6tjodVi5d7W9/JUl6d005aUKMNwzW2/YgU88IBrGKb09CsfV6uWq1U1KsrVlWD4cNeyYUPXIPciImXB4cOHmTNnDmlpafj6+tKtWzciL57qrhxRMSseYRgGPyf8zOSfJjN/93x38ZqrSlAVWldvzftD3qdB5SJM3SNyGYYBhw65Wli3bIFp01zLi/2+kI2Kct3Fn5gIPXvC7bdDs2Zeiywi4hFOp5MffviB7777DsMwiIqKYvTo0eW2kAUVs+IBJ1JOMPCzgWxP3J5vuwULdSPq8vfef2dUs1H4WvXtJ1d25ozrkZHhGnd17VrXDVihoTB1quuO/JSUK5+nY0f45BPXTVi6i19EyqO0tDTmzp3LoUOHAGjdujUDBw4sNRMheIqqCSlRr37/Kv9Y+w/SbGkAtK3RlnY12vFijxepFVbL5HRSmhmGa7an/ftd465u3gyzZl35uIsL2UqVIDraNTXp+PHw5JNQhodOFBEpMsMwmDFjBmfOnMHPz4/BgwfTqlUrs2N5hYpZKRGGYXDfgvuYtnWae9sP9/xAtzrdTEwlpZFhwM8/u2ae+ukn19Si77135T6tMTGuGaMsFtd+tWvDwIGu9dtvd/V1VZ9WEamoLBYLffr0YeXKlcTFxREVFWV2JK9RMSvXLDU7lTqT63Ah6wIAlfwrcfCxg0QFV5z/SOJiGHDuHHz3Xd50pi+/nDeg/8aNlz42t5CtUcNVmLZv75qdKjraNf6qiIjkl5qaSlJSEnV/G5S6UaNGNGzYEKu1Yk0kpGJWrolhGNw+93Z3Idsztier7lplbijxijNnXLNevfCCqz/q118X/xwjR7q6FQwe7Cpix41zTRogIiKXt3//fubNm4fT6WTcuHFEREQAVLhCFlTMyjXYeGIjY+eOZX/SfgBe7fUqz934nMmpxFNSU+Gzz+CJJ1yzVV3sUoVs166u1trc/q8+Pq5xWrt3d3UZEBGR4nE6naxcuZK1a9cCUL16dZxOp8mpzKViVq7Kaz+8xsvfvYzNYcNqsfLCjS+okC0ncnLg119dEwzk5LgmFjh3Dt5//9LHxMS4Wmizs2HMGNewVyIiUrKSk5OZM2cOx44dA6B9+/b0798f3wo+5WDFfvdyVf684s9MWjMJgH4N+vHpiE+pGlLV5FRytZ57Dl57zVWQgmskgMsZOBDq14cHH4Trr8/rGysiIp6zd+9e5s+fT2ZmJgEBAQwdOpTmzZubHatUUDErxfLmujfdhSzA/DHzCfILMjGRXElOjmsSgcOH4e23XRMDbN3q2n6x3xexQUGu0QEcDqhb11X09urlGt9VRES8a9++fWRmZlKzZk3i4uLK9SQIxaViVorEaTi5e/7dfLLtEwD8rH5k/SULq6XidTQvK86ccbW2Zmfn336pEQX+8hcYNsy1HhPjmlxARERKh/79+xMREUHHjh0rfLeC39NXQ64oIS2BNu+3ISEtwb3t0GOHVMiWQufPw1tvuYbDKsyf/gRVq0LTpq4W18aNXbNhaWIBEZHSZffu3Wzbto24uDisViu+vr507drV7FilkopZuaykzCQGfDrAXci+3PNlnr/xeSzqKFkqHDoEn38O27dferaszp1h5UoIDPRuNhERKT673c7y5cvZsGEDAD///DPt2rUzOVXppmJWLulcxjl6z+jNL6d/oZJ/Jb6981s61OpgdiwBXn/dNYvWF19cep+nn3Z1HahUyXu5RETk6iUlJREfH8+pU6cA6Ny5M61btzY3VBmgYlYKZXPYaP1+a46nHCc6JJqVd66kebTumjTT4cNw992u5ZEj+V/r3NnVdeDWW6FvXxPCiYjINfn111/5+uuvyc7OJigoiOHDh9OoUSOzY5UJKmalAIfTQYv3WnA85TgA8aPjVcia6N//hjlz4LfxsfN5/nm4/XZX31cRESmbfvjhB1auXAlATEwMo0aNIjw83ORUZYeKWSnghVUvsC9pHwDPdn2W7nW7m5yo4tm1yzUU1rx5BV9r2BC++Qbq1fN+LhERKXmNGjXihx9+oGPHjvTq1atCTkl7LSyGYRhmh/CmlJQUwsPDSU5OJiwszOw4pc4vCb/Q+v3WAPy525/5W++/mRuoAsjJgcceg//+9/L7/fGPrskNQkK8k0tERDzn3LlzVKlSxf08NTWVSrrJwa049ZpaZsUt257N2LljAWhcpTF/vemvJicqv7ZuhW3b4K67Lr/fkCGubgYNG3olloiIeFhOTg5Lly5l69at3HPPPdSuXRtAhew1UDErbq9+/yo7z+wE4IOhH2gc2RJiGK6HzQbr10PPnpfe9403XLNsNWgA6i4lIlK+nDlzhvj4eBITEwE4ceKEu5iVq6diVgA4k36GqVunAnBP63u4se6NJicqH4YMgUWLLv16vXrg6wtff62buEREyrOtW7eyePFicnJyCAkJYeTIkdSvX9/sWOWCilkB4LY5t3Ey9SSVgyozecBks+OUaW+8ATNnwubNBV+rUgW6dIE6deCdd7yfTUREvMtms7F48WJ++eUXAOrVq8fIkSMJDQ01OVn5oWJW+P7I96w4tAKAV3u9SliAboy7GomJUK1a4a8dOQKhoRAZCZo8TUSk4tixYwe//PILFouFnj170q1bN41WUMJUzFZw6bZ0+n/aH4DudbrzUPuHTE5UdjidkJDgKmLfeQc+/jj/6w89BAMGwLBh5uQTERHztWnThhMnTnD99dcTGxtrdpxyScVsBTd+yXiy7FkATBs2DYuaDa/oiy9ckxjMmVP469ddB3v3ejeTiIiUDtnZ2Xz//ffceOONBAQEYLFYGDp0qNmxyjUVsxXY0eSjTN86HYC7W99Ng8oNzA1Uiu3bB//8J3z0UcHXgoOhSRMICoKhQ+GZZ7yfT0REzJeQkEB8fDznzp0jPT2d4cOHmx2pQlAxW4HdNT9vkNN3BupupEvZsgXatSu4/f33YdQo101dIiJScRmGwebNm1m6dCkOh4OwsDDatm1rdqwKQ8VsBbX33F5WH14NwKcjPiXEX9NK5XI64csvXUNqffpp/tcCA+H22+HDD3Ujl4iIQFZWFgsXLuTXX38FXFPTDhs2jODgYJOTVRwqZisgp+Hk/gX3A9AiugVjrx9rcqLSISEBli6Fe+4p/PVFi2DQIO9mEhGR0isxMZGZM2dy/vx5rFYrffr0oVOnTrr/xMtUzFZAL656kR+O/kCwXzBf3/Z1hf1PZ7e7JivYuBGWLHFNMft7gwa5itvhw12TG4iIiOQKDg7GZrMRHh5OXFycZvMyiX49VzC/Jv7Kqz+8CsD4G8YTGxFrbiATGAZcaoi/9u1h4EC44w5o1Mi7uUREpPTLycnBz88PgNDQUG6//XYiIiIICgoyOVnFpWK2AnEaTtq83waAuuF1+Xufv5ucyPsWL4bBg/Nvu+EG+OMfoV8/iI42J5eIiJR+x48fJz4+nj59+tCiRQsAatSoYXIqUTFbgczcMZMcZw4AU4dNrXDdCwYMgGXL8m/LzgZ/f3PyiIhI2WAYBj/99BPffvstTqeTtWvX0rx58wr3e7S00nxqFYTTcPLokkcBaFylMTfVu8nkRN7z9ddQvXr+QnbCBNeoBSpkRUTkcjIyMpg5cybffPMNTqeTZs2acdddd6mQLUXUMltBLN2/lKTMJAAW377Y5DTe8+GH8OCD+bft2uWa5EBERORyjh07Rnx8PCkpKfj4+DBgwADatWunQraUUTFbQQz5fAgAtzS/hfqR9U1O4x0BAWCz5T1/6y1X31gREZErOX/+PNOnT8fpdFK5cmVGjx5N9erVzY4lhVAxWwFsObUFAwOAl3q8ZG4YL1i/Hjp1yr/t2DHQiCkiIlJUkZGRdOzYkbS0NAYPHkxAQIDZkeQSVMxWAO9ueBeAtjXa0rRqU5PTeNaBAwULWadTs3WJiMiVHT58mMjISMLDwwHo06cPFotF3QpKOd0AVs4ZhsHUrVMBeLbrsyan8azUVGjYMO95v34qZEVE5MqcTiffffcdM2bMID4+HofDAYDValUhWwaomC3nZu+c7V6/ufHNJibxDMOAxER46ikIC8vb/swzrtEL9DNIREQuJy0tjU8//ZTVq1djGAZVqlTB6XSaHUuKQd0MyjHDMJi0ZhIATaKaEOBbvvr7xMbCkSMFtz/4IPy94s0HISIixXTo0CHmzJlDeno6fn5+DBo0iNatW5sdS4pJxWw5tuHEBrYmbAXg9b6vmxumhHXqVHghu2kTtGvn/TwiIlJ25HYr+P777wGIjo4mLi6OqlWrmpxMroaK2XIsfmc8AC2rtWRIoyEmpyk5//mPa8SCXEeOQJ065uUREZGyxel0smfPHgDatGnDwIED8fPzMzmVXC0Vs+WUYRhM/2U6AH/p/hdzw5QAw4CJE+G991w3euVauVKFrIiIFI+vry9xcXGcOnWK66+/3uw4co1UzJZT8TvjOZtxlmC/YAZeN9DsONdk/3647rqC2xcvhl69vJ9HRETKFqfTycqVK/H39+fGG28EICoqiqioKJOTSUlQMVtOxe9ydTEY03wMof6hJqe5ehs3QocO+bd98gnccYc5eUREpGxJTk5mzpw5HDt2DIvFQvPmzalSpYrZsaQEqZgth85nnmfh3oUA3NvmXpPTXL1Zs+DWW/Oed+wIa9aAr75rRUSkCPbu3cv8+fPJzMwkICCAoUOHqpAth1QWlEPvbnyXjJwMro++nq4xXc2OU2wOh2vCg5Ur87a99Rb88Y/mZRIRkbLD4XCwYsUK1q1bB0CNGjWIi4ujcuXKJicTT1AxW844DSd//f6vADza4dEyN3NJVhYEBeXfdvAg1KtnTh4RESlbDMPg008/5fDhwwB06NCBvn374quP9cot/cuWM6sPr8bmsAFw2/W3mZymeAorZNetUyErIiJFl9svNiEhgZtvvpmmTZuaHUk8TMVsOTN963QA2lRvU6Zu/Fq/3jURwsUMw5wsIiJSttjtdlJSUtzdCNq1a0eTJk0IDS07vwfl6lnNDiAl66s9XwFwZ6s7TU5SdFlZ+QtZHx9Xv1kREZErOX/+PFOnTmXGjBlkZmYCrtZZFbIVh4rZcmTXmV2kZKcAlKkZvy6eBvuVV8BuB6u+M0VE5Ap27tzJ+++/z6lTp7DZbJw7d87sSGICdTMoR97Z8A4Azao2o2HlhianuTKnEx56CH6bURCA5583L4+IiJQNdrudZcuWsWnTJgBiYmIYNWoU4eHhJicTM6iYLUeWHVgGwNBGQ01OUjQ+Pvmf799vTg4RESk7zp07R3x8PAkJCQB07dqVXr164fP7XypSYaiYLSeOpxznwPkDWLAwsdtEs+Nc0bvv5n9+4ADUr29OFhERKTtWr15NQkICwcHBjBgxgoYNS/8nkeJZKmbLiQV7FrjXwwNL/8cs48fnrTudUMaGwxUREZMMHDgQgL59+xIWFmZyGikNdJtNOfHhlg+BsjG27IgReeuvv65CVkRELu3MmTOsWrUK47fxGoODgxk1apQKWXFTy2w54DScbE3YCkDPuj1NzXIl//kPzJ+f9/zJJ02LIiIipdwvv/zCokWLyMnJoXLlyrRq1crsSFIKqZgtBzae2Ohe/0OrP5iY5PKcTvjjH/Oe//STWmVFRKQgm83GkiVL2Lp1KwD16tWjQYMG5oaSUkvFbDnw303/BeD66OsJ9A00Oc2l1a6dt75lC7RpY14WEREpnRITE5k9ezZnz57FYrHQo0cPunfvjlUDkMslqJgtB+btngfAhM4TTE5SkMMBCxbAW2/BqVN521XIiojI723fvp0FCxZgt9sJDQ1l1KhRxMbGmh1LSjkVs2XcqdRT7lm/+jfob3Ka/LZuzV+05v5Rffy4KXFERKSUCwkJwW6306BBA0aMGEFISIjZkaQMUDFbxn1z4BsAGlVpRI1KNUxOk9/FhewTT8CECfm7GoiIiNhsNvz9/QGoX78+d999N3Xq1MGimyqkiNQBpYxbe2wtAD6W0jXzyebNeesTJsCbb6qQFRGRPIZhsGnTJt566y2SkpLc2+vWratCVopFxWwZZnfa3f1lX+zxoslp8vz0E7Rvn/f8jTfMyyIiIqVPdnY2c+bMYdGiRWRkZLBp0yazI0kZZnox++677xIbG0tgYCAdO3Zkw4YNl91/8uTJNG7cmKCgIGJiYnjiiSfIysryUtrSZdvpbZzNOEuofyijmo0yOw4As2ZB5855zydO1PBbIiKS5+TJk7z//vv8+uuvWK1W+vbtS9++fc2OJWWYqX1mZ82axYQJE5gyZQodO3Zk8uTJ9O/fnz179hAdHV1g/88//5xnn32WqVOn0qVLF/bu3cvdd9+NxWLhzTffNOEdmGvN0TUAhAWE4Ws1v/tzZibcemve87VroUsX8/KIiEjpYRgGGzZsYPny5TgcDsLDw4mLi6O2+qDJNTK1ZfbNN9/kgQce4J577qFZs2ZMmTKF4OBgpk6dWuj+P/74I127dmXs2LHExsbSr18/brvttiu25pZXb/zo+vz+ma7PmJzEZdCgvPUPPlAhKyIiebZu3crSpUtxOBw0adKEcePGqZCVEmFaMWuz2di8eTN9+vTJC2O10qdPH9atW1foMV26dGHz5s3u4vXgwYMsXryYQRdXUb+TnZ1NSkpKvkd5sO/cPo6lHAPg1ha3XmFvz8vOhtWrXeu1asEDD5gaR0RESpmWLVtSp04dBgwYwC233EJQUJDZkaScMO2z6bNnz+JwOKhWrVq+7dWqVWP37t2FHjN27FjOnj1Lt27dMAwDu93OQw89xJ///OdLXmfSpEm8/PLLJZq9NNhyaot7PTqkYJcMbwu8aOKxX381L4eIiJQOhmGwfft2mjdvjo+PDz4+Pu6ugSIlyfQbwIpj9erVvPbaa7z33nts2bKFuXPnsmjRIv76179e8piJEyeSnJzsfhw7dsyLiT3n7Q1vA9C5ducr7Ol5GRn5n4eHm5NDRERKh8zMTGbOnMm8efNYtWqVe7sKWfEE01pmo6Ki8PHx4fTp0/m2nz59murVqxd6zPPPP88f/vAH7r//fgCuv/560tPTefDBB3nuuecKnbc5ICCAgICAkn8DJvvx2I8A3H797SYngbZt89YTEszLISIi5jt27Bjx8fGkpKTg4+NDuFo4xMNMa5n19/enXbt2rFixwr3N6XSyYsUKOncuvLUxIyOjQMHq4+OaLMAwDM+FLWVOpJxwr49uPtrEJK6paffsyXv+u14jIiJSQRiGwZo1a5g2bRopKSlUrlyZ+++/nxtuuMHsaFLOmTqe04QJE7jrrrto3749HTp0YPLkyaSnp3PPPfcAcOedd1KrVi0mTZoEwNChQ3nzzTdp06YNHTt2ZP/+/Tz//PMMHTrUXdRWBN8e/BaAxlUam9pf1maDmJi85xcumBZFRERMlJ6ezvz589m/fz8ALVq0YMiQIeXyk1EpfUwtZseMGcOZM2d44YUXSEhIoHXr1ixdutR9U9jRo0fztcT+5S9/wWKx8Je//IUTJ05QtWpVhg4dyt/+9jez3oIplh9cDsCIJiNMzXHxz6jBg9VXVkSkosrMzOTIkSP4+voycOBA2rRpo/6x4jUWoyJ9Pg+kpKQQHh5OcnIyYWFhZse5KrGTYzmSfITlf1hOn/p9rnyAB0yeDE88kfe8Yn0XiYjI7+3evZvIyMgCoxSJXI3i1GtlajQDgcMXDnMk+QhWi5WOtTqakmHIkLxCtm9fcDhMiSEiIiZJS0vj008/5ciRI+5tTZo0USErplAxW8Ys3b8UgBbRLagUUMnr13c6YdGii/IshUIGkRARkXLq4MGDTJkyhQMHDrBgwQKcTqfZkaSCM7XPrBTfV3u+AqBOeB1Trr90ad76vn0qZEVEKgqn08l3333H999/D0DVqlUZPXp0ocNiiniTitkyZt+5fQCMbDLSlOsPHuxa+vtDw4amRBARES9LTU1l7ty5HD58GIA2bdowcOBA/Pz8zA0mgorZMsXmsHHowiEAetXr5fXrXzzL8MSJXr+8iIiYIDk5mQ8++ICMjAz8/PwYMmQILVu2NDuWiJuK2TJk55mdOA0nlfwrUTe8rlev7XBA06au9YAAeOklr15eRERMEhYWRr169Th79iyjR4+mSpUqZkcSyUfFbBmy5ugaANrXbO/18fv6XDQC2FNPefXSIiLiZSkpKfj7+xMYGIjFYmHo0KFYrVZ1K5BSSb22y5CVh1YC0K5GO69e9+BBWL067/mLL3r18iIi4kV79+5lypQpLFiwwD1VfEBAgApZKbXUMluGbE/cDkD9yPpevW6DBnnrp0+Dfp6JiJQ/DoeDFStWsG7dOgAuXLhAdnY2gYGBJicTuTwVs2XEiZQT7E9yzXk9pNEQr133p5/y1hs2hOhor11aRES85MKFC8yZM4fjx48D0KFDB/r27Yuvr8oEKf30XVpGLNm/BAALFmLCY7x23e7d89b37fPaZUVExEt2797NV199RVZWFgEBAQwbNoymuXf8ipQBKmbLiPXH1wPwRKcnvHZNpxPsdtd6jx5eu6yIiHhJTk4OS5YsISsri1q1ajFq1CgiIyPNjiVSLCpmy4gNJzcA0K1ON69d87nn8tYXLvTaZUVExEv8/PwYNWoUu3fvpnfv3vj4+JgdSaTYVMyWAWm2NHYk7gCgY+2OXrvu3//uWgYEQGio1y4rIiIetHPnTux2u3vigzp16lCnjjlTpIuUBBWzZcDmk5txGk5qh9WmZqWaXrnm3r1566+/7pVLioiIB9ntdpYtW8amTZvw9fWlVq1amgBBygUVs2XAd0e+A6BtjbZeu+Zbb+Wtjx/vtcuKiIgHnDt3jvj4eBISEgDo2LEjERER5oYSKSEqZsuA3Jm/YsNjvXbN995zLdu0AS9PNiYiIiVox44dfP3119hsNoKDgxk+fDjXXXed2bFESoyK2TLA1+r6Z7I77V65Xmpq3vr//Z9XLikiIiXMMAwWLVrE5s2bAVff2FGjRhEWFmZyMpGSpWK2DNhzbg8AI5uO9Mr1OnfOW7/vPq9cUkRESpjFYiE4OBiA7t2707NnT6xWzWIv5Y+K2VLO5rBx+MJhAJpW9c4g1tHR8OuvrnV1MRARKVtsNhv+/v4A9OzZk+uuu46YGO9NtiPibfoTrZQ7fOEwTsNJsF8wNUJrePx6y5fDqlWu9a++8vjlRESkhNhsNr766iumT5+O/bcZb6xWqwpZKffUMlvK7TyzE4BGVRph8UIzab9+eeuDBnn8ciIiUgISExOJj4/nzJkzWCwWDh8+TMOGDc2OJeIVKmZLuUV7FwHQpnobj1/rzJm89RdeAF99d4iIlGqGYbB161YWL16M3W4nNDSUUaNGERsba3Y0Ea9RuVLK7TjjmvmredXmHr/W6NF568884/HLiYjINcjOzmbRokVs374dgAYNGjBixAhCQkJMTibiXSpmS7mfjv8EQPNozxez37nmZiAiAn67AVZEREqphQsXsmPHDiwWC7169aJbt25e6Y4mUtqomC3FbA6be/366Os9eq34+Lz1WbM8eikRESkBN910E6dPn2bIkCHUqVPH7DgiprEYhmGYHcKbUlJSCA8PJzk5udQPHL377G6avusajsv5gtOjf3FffGqnU0NyiYiUNtnZ2ezfv5/mzfM+qTMMQ62xUi4Vp15Ty2wplju+bLOqzTz6w2rhwrz1t95SISsiUtqcOnWK2bNnc/78eQICAtwjFaiQFVExW6odSDoAQMPKnh1eZdSovPVHH/XopUREpBgMw2Djxo188803OBwOwsPDCQwMNDuWSKmiYrYU23JqCwANIz1XzBoG2H7rmvvEE2qVFREpLbKysliwYAG7du0CoHHjxgwbNoygoCCTk4mULipmS7GlB5YC0KByA49do3r1vPWnn/bYZUREpBhOnDhBfHw8Fy5cwGq10rdvXzp27KhuBSKFUDFbitmdrukI60fW98j5L1yAxMS85xcXtiIiYp6zZ89y4cIFIiIiiIuLo1atWmZHEim1VMyWUln2LBLTXZXmDTVv8Mg1IiPz1nNyPHIJEREpootHJmjVqhU2m43rr79efWRFrsBqdgAp3NHkowCE+IVQOahyiZ8/PT1vfeRITV0rImKmY8eOMXXqVDIyMtzbbrjhBhWyIkWgYraUOnLhCACxEbEe6SPVtWve+uzZJX56EREpAsMwWLt2LdOmTeP48eOsXLnS7EgiZY7a40qp3DFm60bULfFzOxzwyy+u9cGDwao/aUREvC49PZ358+ezf/9+AFq0aEHfvn1NTiVS9qiYLaX2J7l+uNWPKPmbvyZMyFtXq6yIiPcdOXKEOXPmkJqaiq+vLwMGDKBt27YarUDkKqiYLaWOpRwDXN0MStoPP+Sta7hCERHv2r17N19++SWGYVClShVGjx5NtWrVzI4lUmapmC2lDl04BECtsJIfjsXhcC0nTSrxU4uIyBXExsYSERFBTEwMgwcPxt/f3+xIImWaitlS6qfjPwFwXeXrSvS8Bw/Ctm2u9T/8oURPLSIil3D69Gmio6OxWCwEBgZy//33ExQUpG4FIiVAt/6UQmm2NPd6vch6JXruBr9NJta5M2gMbhERz3I6naxevZopU6awadMm9/bg4GAVsiIlRC2zpdCuM7vc6yU5xuzFY8tqti8REc9KTU1l7ty5HD58GIDEi6dcFJESo2K2FNqXtA+AG+veWKLnfeihvPUvvijRU4uIyEUOHDjAvHnzSE9Px8/PjyFDhtCyZUuzY4mUSypmS6G95/YCJd9fdvly17J5cwgIKNFTi4gIed0Kfvht2Jhq1aoRFxdHVFSUyclEyi8Vs6VQbstsSRazP/4Ip0+71r/5psROKyIiFzl9+jRr1qwBoF27dvTv3x8/Pz+TU4mUbypmS6F9534rZquUXDH79NN56zVrlthpRUTkIjVq1KBv375UqlSJFi1amB1HpEJQMVvKGIZR4i2zP/wAP7lG+mLYsBI5pYiIAA6Hg9WrV9OyZUuqVq0KQOfOnU1OJVKxaGiuUuZc5jkuZF0AoEHlBtd8vtGj4cYbXRMlhITAJ59c8ylFRARITk5m+vTprFmzhvj4eBy5M9KIiFepZbaUye1iUDusNsF+wdd0LsPIa5EFV5/ZkJBrOqWIiAB79uxh/vz5ZGVlERAQQI8ePfDx8TE7lkiFpGK2lCnJLga7dsHx4671CxdUyIqIXCuHw8Hy5ctZv349ADVr1iQuLo7IyEiTk4lUXCpmSxn3zV8lUMz++9+u5aBBEB5+zacTEanQ0tPT+fzzzzl58iQAnTp1ok+fPmqRFTGZitlSJrdltlGVRtd8rmXLXMuGDa/5VCIiFV5QUBC+vr4EBgYyfPhwGjdubHYkEUHFbKmz8tBKoGSG5cptLOjY8ZpPJSJSIdntdiwWCz4+PlitVkaNGoXT6SQiIsLsaCLyG41mUIoYhsGZjDMAxEbEXtO5jh6F36YDZ9Cga8slIlIRJSUl8fHHH7M8d/pEICwsTIWsSCmjltlS5GjyUff6tXYzmDcvb10/d0VEimfHjh18/fXX2Gw2UlJSuPHGGwkOvrYRZkTEM1TMliJHko+41wN9A6/pXLNnu5ZNm17TaUREKpScnByWLl3Kli1bAKhTpw6jRo1SIStSiqmYLUVOpJwA4Ma6N17zudaudS1btrzmU4mIVAhnz55l9uzZJCYmAtC9e3d69uyJ1aoeeSKlmYrZUuTg+YMA1I+sf03nyS1kAf7v/67pVCIiFYLdbmfGjBmkpqYSEhLCiBEjaNDg2mdhFBHPu6ZiNisri8DAa/s4XPLkFrP1Iupd03kuLmBvvPZGXhGRcs/X15f+/fuzadMmRo4cSaVKlcyOJCJFVOzPTpxOJ3/961+pVasWoaGhHDzoKsCef/55Pv744xIPWJEcunAIuLaW2TNnYNs21/p//lMSqUREyqfExESOHMm7V6F58+bceeedKmRFyphiF7Ovvvoq06dP55///Cf+/v7u7S1atOCjjz4q0XAVTW4xey0ts19+mbeuLgYiIgUZhsHPP//Mhx9+yJdffklqaqr7NYvFYmIyEbkaxS5mZ8yYwQcffMDtt9+ebwq/Vq1asXv37hINV5EYhsHhC4cBqBVW66rPk/v3RFgY6J4FEZH8bDYb8+fPZ8GCBdjtdqpXr64bvETKuGL3mT1x4gQNC5kf1el0kpOTUyKhKqKU7BT3etXgqld1DsOArVtd62PHlkAoEZFy5PTp08yePZtz585hsVjo1asX3bp1U2usSBlX7GK2WbNm/PDDD9StWzff9vj4eNq0aVNiwSqa3AkTwgPCCfEPuapzfP553vqTT5ZEKhGRss8wDLZs2cLSpUux2+1UqlSJUaNGFfg9JiJlU7GL2RdeeIG77rqLEydO4HQ6mTt3Lnv27GHGjBksXLjQExkrhL3n9gLXNvPXJ5/krRfSeC4iUiFZLBaOHTuG3W6nYcOGjBgxQpMgiJQjxS5mhw0bxtdff80rr7xCSEgIL7zwAm3btuXrr7+mb9++nshYIXxz4BsA/H38r7DnpR0/7loOG1YSiUREyjbDMNxdCAYNGkTt2rVp166duhWIlDNXNc5s9+7dWb58eUlnqdB2nd0FQJOoJld9jpTfut3edVdJJBIRKZsMw2Djxo0cPnyY0aNHY7FY8Pf3p3379mZHExEPKPYtnPXr1+fcuXMFtl+4cIH69a9t5qqKzMfqGhmiedXmV3V8YiIcO+Za7927pFKJiJQtWVlZxMfHs2TJEnbt2sWuXbvMjiQiHlbsltnDhw/jcDgKbM/OzubEiRMlEqoiOnTeNcZsq+qtrur4f/3LtWzY0DUsl4hIRXPixAni4+O5cOECVquVvn370rRpU7NjiYiHFbmYXbBggXt92bJlhIeHu587HA5WrFhBbGxsiYarSI4ku2ahqRxUudjH2mzwz3+61mvUKMlUIiKln2EYrF+/nuXLl+N0OomIiCAuLo5ata5+zG4RKTuKXMwOHz4ccN0VetfvOmX6+fkRGxvLv3KbB6VYsu3Z7vVqIdWKffw77+StT5lSEolERMqOJUuWsHHjRgCaNm3KzTffTGBgoMmpRMRbilzMOp1OAOrVq8fGjRuJioryWKiKJnfmL4DqodWLffw33+StN2tWAoFERMqQVq1a8csvv9C7d29uuOEGjVYgUsEUu8/soUOHPJGjQjuRmtfX+Gp+CPv+9q/4hz+UVCIRkdLLMAxOnz5N9equP/5r1arF448/TlBQkMnJRMQMVzUhdXp6OosXL2bKlCm8/fbb+R7F9e677xIbG0tgYCAdO3Zkw4YNl93/woULPPLII9SoUYOAgAAaNWrE4sWLr+ZtlBo7z+wEoGdsz2IfaxiwaJFr/b77SjCUiEgplJGRwRdffMFHH31EQkKCe7sKWZGKq9gtsz///DODBg0iIyOD9PR0KleuzNmzZwkODiY6Opo//vGPRT7XrFmzmDBhAlOmTKFjx45MnjyZ/v37s2fPHqKjowvsb7PZ6Nu3L9HR0cTHx1OrVi2OHDlCREREcd9GqZKZkwnAuYyCQ55dycUDSLS6uoEQRETKhCNHjjBnzhxSU1Px8fHh7Nmz7tZZEam4il3MPvHEEwwdOpQpU6YQHh7OTz/9hJ+fH3fccQePPfZYsc715ptv8sADD3DPPfcAMGXKFBYtWsTUqVN59tlnC+w/depUkpKS+PHHH/Hz8wMoFyMo7EvaB0DbGm2LfezFX/IyXtOLiBTKMAzWrFnDqlWrMAyDKlWqMHr0aKpVK/4NsyJS/hS7m8HWrVt58sknsVqt+Pj4kJ2dTUxMDP/85z/585//XOTz2Gw2Nm/eTJ8+ffLCWK306dOHdevWFXrMggUL6Ny5M4888gjVqlWjRYsWvPbaa4WOe5srOzublJSUfI/SysfiU+xjVq1yLTXNuIiUR+np6Xz22WesXLkSwzBo2bIlDz74oApZEXErdjHr5+eH1eo6LDo6mqNHjwIQHh7OsdwpqIrg7NmzOByOAj+QqlWrlq8f1MUOHjxIfHw8DoeDxYsX8/zzz/Ovf/2LV1999ZLXmTRpEuHh4e5HTExMkTN6y5mMMwDcUOuGYh97/rxr+fe/l2QiEZHSYdu2bRw4cABfX19uvvlmhg8fjr+/v9mxRKQUKXY3gzZt2rBx40auu+46evTowQsvvMDZs2f55JNPaNGihScyujmdTqKjo/nggw/w8fGhXbt2nDhxgtdff50XX3yx0GMmTpzIhAkT3M9TUlJKXUGb21e2uBMmHDiQtz5sWEkmEhEpHTp16kRSUhI33HBDofdSiIgUu2X2tddeo8Zv00z97W9/IzIykocffpgzZ87w/vvvF/k8UVFR+Pj4cPr06XzbLx5u5fdq1KhBo0aN8PHJ+zi+adOmJCQkYLPZCj0mICCAsLCwfI/SJikzCYAqQVWKddz//V/eep06JZlIRMQcqampLFy4kJycHMA1XOHgwYNVyIrIJRW7ZbZ9+/bu9ejoaJYuXXpVF/b396ddu3asWLHCPbuY0+lkxYoVjB8/vtBjunbtyueff47T6XR3ddi7dy81atQo0x87Xci6AEBEYESxjluxwrVUISsi5cGBAweYN28e6enpWK1WBg0aZHYkESkDrmqc2cJs2bKFIUOGFOuYCRMm8OGHH/K///2PXbt28fDDD5Oenu4e3eDOO+9k4sSJ7v0ffvhhkpKSeOyxx9i7dy+LFi3itdde45FHHimpt2GK3EkTKgVUKtZxufe9vfZaSScSEfEep9PJypUr+fTTT0lPTyc6OpoOHTqYHUtEyohitcwuW7aM5cuX4+/vz/3330/9+vXZvXs3zz77LF9//TX9+/cv1sXHjBnDmTNneOGFF0hISKB169YsXbrUfVPY0aNH3S2wADExMSxbtownnniCli1bUqtWLR577DGeeeaZYl23NLE77e71Sv5FL2Zzb/wCGDiwJBOJiHhPSkoKc+bMcd9M3LZtWwYMGOAeflFE5EoshmEYRdnx448/5oEHHqBy5cqcP3+eKlWq8Oabb/Loo48yZswYHnvsMZo2berpvNcsJSWF8PBwkpOTS0X/2VOpp6j5Zk0A7M/b8bEWbXiu776Dnj1d60X7FxQRKV2OHj3KrFmzyMjIwN/fn6FDh3r8RmIRKRuKU68VuWX2rbfe4h//+Ad/+tOfmDNnDqNHj+a9995j+/bt1K5d+5pDV1QnU0+614tayAJ8/rlrabGUdCIREe8IDw/HMAyqV69OXFwcVaoU7yZYEREoRjF74MABRo8eDcDIkSPx9fXl9ddfVyF7jfYn7S/2MQ4HfPCBa/3550s4kIiIB2VlZREYGAi4itk777yTqKgofH2LfT+yiAhQjBvAMjMzCf5tmimLxUJAQIB7iC65euk56QDERsQW+Zjt2/PWizmDsIiIafbs2cPbb7/Nnj173NuqV6+uQlZErkmxfoJ89NFHhIaGAmC325k+fTpRUVH59vnjH/9YcukqgF1ndgFwffT1RT7m11/z1isXb54FERGvczgcfPvtt/z0008AbNy4kcaNG5ucSkTKiyIXs3Xq1OHDDz90P69evTqffPJJvn0sFouK2WI6nnq82Md8/71r2ahRCYcRESlh58+fZ86cOZw44RqCsGPHjvTt29fkVCJSnhS5mD18+LAHY1RcPhbXTV/1IuoV+Zjcyc5+6/UhIlIq7dq1i6+++ors7GwCAwMZNmwYTZo0MTuWiJQz6qhksoS0BADa12x/hT3zTJ/uWo4c6YFAIiIl4NSpU3z55ZcA1K5dm1GjRhEREWFuKBEpl1TMmiwxPRGAaqHVirT/b13OABg71hOJRESuXY0aNWjfvj3+/v7cdNNN+PgUfehBEZHiUDFrstPppwGoFlK0YvbTT13LunWhQQNPpRIRKb6dO3dSp04d943CgwYNwqLBsEXEw4o8NJeUPKfhdLfMRodEF+mYgwddy9atPRRKRKSYcnJyWLhwIbNnz2bu3Lk4nU4AFbIi4hVqmTVRUmaSe71KcNFmvjn+2+AH/ft7IpGISPGcPXuW+Ph4Tp92fcpUq1YtkxOJSEVzVcXsgQMHmDZtGgcOHOCtt94iOjqaJUuWUKdOHZo3b17SGcutM+lnAPC1+uLv43/F/R2OvAkTbrrJk8lERK5s27ZtLFy4kJycHIKDgxk5ciQN1P9JRLys2N0MvvvuO66//nrWr1/P3LlzSUtLA+CXX37hxRdfLPGA5Vluf1m7016k/deuzVvX7wsRMUtOTg4LFixg3rx55OTkEBsby0MPPaRCVkRMUexi9tlnn+XVV19l+fLl+PvntSbedNNN7tldpGhyW2a71elWpP2XLHEtfXxAsz+KiFkMw+DYsWMA9OjRgz/84Q9UqlTJ5FQiUlEVuyTavn07n3/+eYHt0dHRnD17tkRCVRTnMs8BEBUcdYU9XX74wbVs29ZTiURELs0wDCwWC/7+/sTFxZGenk79+vXNjiUiFVyxW2YjIiI4depUge0///yzOv4X07kMVzFbJahoN3/ldjNo08ZTiURECrLZbMyfPz/fp2/VqlVTISsipUKxi9lbb72VZ555hoSEBCwWC06nk7Vr1/LUU09x5513eiJjuXU2w9WSXZRiNilv4AOGDvVUIhGR/E6fPs2HH37IL7/8wsqVK933SYiIlBbF7mbw2muv8cgjjxATE4PD4aBZs2Y4HA7Gjh3LX/7yF09kLLdyuxkUZViu11/PWx882FOJRERcDMNgy5YtLF26FLvdTqVKlRg1apR7QgQRkdKi2MWsv78/H374Ic8//zw7duwgLS2NNm3acN1113kiX7mWO2FC1eCqV9z37393Lf38QOOQi4gnZWdns3DhQnbs2AFAw4YNGT58OCEhISYnExEpqNjF7Jo1a+jWrRt16tShTp06nshUYZzJcI1mUDXkysVsrscf91AYERHA4XDw8ccfc+bMGSwWC71796ZLly6azUtESq1i95m96aabqFevHn/+85/ZuXOnJzJVGFsTtgJQLaTaZfe7eJCIiRM9GEhEKjwfHx/atGlDWFgY99xzD127dlUhKyKlWrGL2ZMnT/Lkk0/y3Xff0aJFC1q3bs3rr7/O8dx5VqVIDMPAabjmL48Mirzsvr/84lo2aACRl99VRKTYsrKyOHfunPt5p06dePjhh4mJiTExlYhI0RS7mI2KimL8+PGsXbuWAwcOMHr0aP73v/8RGxvLTZpjtcjSbHl3BNcIrXHZfbdudS01JJeIlLSTJ0/y/vvv88UXX5CdnQ2AxWIhMDDQ5GQiIkVzTfNI1atXj2effZZWrVrx/PPP891335VUrnLvfNZ593qwX/Bl9333XdeydWsPBhKRCsUwDNavX8/y5ctxOp1ERESQmppKQECA2dFERIrlqovZtWvX8tlnnxEfH09WVhbDhg1j0qRJJZmtXDuZehJwFbJX6o926JBrGRvr4VAiUiFkZmayYMECdu/eDUCTJk0YNmyYWmNFpEwqdjE7ceJEZs6cycmTJ+nbty9vvfUWw4YNIzj48q2Lkt+JlBMA2J32y+6Xk5O33q6dJxOJSEVw/Phx4uPjSU5OxsfHh379+nHDDTfoJi8RKbOKXcx+//33/OlPf+KWW24hKirKE5kqhKRM15RelfwrXXa/9evz1hs18mQiEakIvvvuO5KTk4mMjCQuLo6aNWuaHUlE5JoUu5hdu3atJ3JUOAlpCQC0rt76svtNm+Za1qsH1mLfricikt+wYcNYvXo1ffv2Vf9YESkXilTMLliwgIEDB+Ln58eCBQsuu+/NN99cIsHKu+2J2wGoFHD5ltnce+rUX1ZErsbRo0c5cOAAvXr1AiA0NJQhQ4aYnEpEpOQUqZgdPnw4CQkJREdHM3z48EvuZ7FYcDgcJZWtXIsOiQYg0PfSN1wYBhw44FofONAbqUSkvDAMgzVr1rBq1SoMw6BGjRo0adLE7FgiIiWuSMWs0+ksdF2u3p5zewBoVa3VJffZsCFv/b77PJ1IRMqL9PR05s2bx4Hf/hpu2bIl9evXNzmViIhnFLsX5owZM9wDa1/MZrMxY8aMEglVEeSOLZuRk3HJfebOdS179YLKlb2RSkTKusOHDzNlyhQOHDiAr68vN998M8OHD8ff39/saCIiHlHsYvaee+4hOTm5wPbU1FTuueeeEglVEew562qZbRJ16Y/9Nm50LS8enktE5FLWrVvHjBkzSEtLIyoqigceeIA2bdpo2C0RKdeKPZqBYRiF/mA8fvw44eHhJRKqIgj1DwXAaVy628aqVa6lZv4SkaKoXLkyhmHQunVrBg4cqNZYEakQilzM5v51b7FY6N27N76+eYc6HA4OHTrEgAEDPBKyPPo54WcA6kcW3o/NMPLWhw3zRiIRKYuysrLcM3c1btyYBx54QGPHikiFUuRiNncUg61bt9K/f39CQ0Pdr/n7+xMbG8uoUaNKPGB5FR4Qzvms85cczeD48bz1zp29FEpEygyn08nq1avZvHkzDz74oPuTMRWyIlLRFLmYffHFFwGIjY1lzJgxmsP7GhiGQaotFYBqIdUK3efdd/PWQ0K8kUpEyoqUlBTmzp3LkSNHANi5cyed9VeviFRQxe4ze9ddd3kiR4WSkp2C3WkHIDIostB9NFyviBRm//79zJs3j4yMDPz9/Rk6dCgtWrQwO5aIiGmKVMxWrlyZvXv3EhUVRWRk5GXvjE1KSiqxcOXV6fTTAFTyr+Qeouv3lixxLV94wVupRKQ0czgcrFq1yj2lePXq1YmLi6NKlSomJxMRMVeRitl///vfVKpUyb2uYV6uzbmMc0DeLGCF+fVX11I3I4sIwPr1692F7A033EC/fv3y3YgrIlJRFekn4cVdC+6++25PZakwEtISADhw/kChr188ruyNN3ojkYiUdjfccAN79uyhY8eONGvWzOw4IiKlRrEnTdiyZQvbt293P//qq68YPnw4f/7zn7HZbCUarrw6mnwUgN71ehf6+sGDeetdungjkYiUNg6Hg02bNrmnEPfz8+Puu+9WISsi8jvFLmbHjRvH3r17ATh48CBjxowhODiY2bNn8/TTT5d4wPIot5tGSnZKoa//8kveuo+PNxKJSGly4cIFpk2bxqJFi/jhhx/c29XFS0SkoGIXs3v37qX1b1NSzZ49mx49evD5558zffp05syZU9L5yqX9SfsBqBtRt9DXd+xwLevU8VYiESktdu3axfvvv8+JEycIDAykWrXCh+8TERGXq5rONvdjr2+//ZYhQ4YAEBMTw9mzZ0s2XTllwdW6cqkJEzZtci3r1fNWIhExm91uZ/ny5WzYsAGA2rVrM2rUKCIiIswNJiJSyhW7mG3fvj2vvvoqffr04bvvvuO///0vAIcOHVILQhFtS9wGQI3QGoW+bncNQUvXrt5KJCJmSkpKIj4+nlOnTgHQuXNnevfujY/6GYmIXFGxi9nJkydz++23M3/+fJ577jkaNmwIQHx8PF10t1KRRAa6JkrwtRb88hsGLF/uWh850pupRMQsNpuNxMREgoKCGD58OI0aNTI7kohImVHsYrZly5b5RjPI9frrr6sVoYhyb/xqEV1w1p7Vq/PWNamPSPllGIb7hq7cCRBq1KhBeHi4yclERMqWqx5xe/PmzezatQuAZs2a0bZt2xILVd6l2lIB1wxgv5fbXxYgIMBbiUTEm86dO8fcuXMZNGgQtWrVAqBJkyYmpxIRKZuKXcwmJiYyZswYvvvuO/eNCRcuXKBXr17MnDmTqlWrlnTGcifNlgZAqH9ogdd+m+CHVq28mUhEvGX79u0sXLgQm83GkiVLuO+++zTklojINSj20FyPPvooaWlp/PrrryQlJZGUlMSOHTtISUnhj3/8oycyljs7z+wEIMQ/pMBriYmuZf363kwkIp6Wk5PDggULmDt3LjabjdjYWMaMGaNCVkTkGhW7ZXbp0qV8++23NG3a1L2tWbNmvPvuu/Tr169Ew5VXkYGRnM86j5/Vr8BruZOo/TaUr4iUA2fOnCE+Pp7E3/5a7dGjBzfeeCNWa7HbE0RE5HeKXcw6nU78/AoWYX5+fu7xZ+XyzmedB6BKcJUCr23d6lr+NkiEiJRxiYmJfPTRR+Tk5BASEsKoUaOop0GkRURKTLGbBW666SYee+wxTp486d524sQJnnjiCXr37l2i4cqjHEeOez3YL7jA6w6Ha1mzprcSiYgnVa1alXr16lGvXj0eeughFbIiIiWs2C2z77zzDjfffDOxsbHExMQAcOzYMVq0aMGnn35a4gHLmwtZF9zruePN5srJq3O57jovBRKREpeYmEhERAT+/v5YLBZGjRqFr6+vuhWIiHhAsYvZmJgYtmzZwooVK9xDczVt2pQ+ffqUeLjyKLeYDfUPxceaf1zeo0fz1msUPjmYiJRihmHw888/s2TJEpo1a8bw4cOxWCz4+/ubHU1EpNwqVjE7a9YsFixYgM1mo3fv3jz66KOeylVu5Q7LFRYQVuC148ddS4sF1IAjUrZkZ2ezaNEi96QyGRkZOBwOfH2vejhvEREpgiL/lP3vf//LI488wnXXXUdQUBBz587lwIEDvP76657MV+7kFrMhfgWH5TpyxLXUSD0iZUtCQgKzZ88mKSkJi8VC79696dKli4bdEhHxgiK3/73zzju8+OKL7Nmzh61bt/K///2P9957z5PZyiX37F8BBWf/SnW9pP6yImWEYRhs3LiRjz76iKSkJMLCwrjnnnvo2rWrClkRES8pcjF78OBB7rrrLvfzsWPHYrfbOXXqlEeClVep2Zeeyvann1xLFbMiZUNWVhbfffcdDoeDRo0aMW7cOPeNsSIi4h1F7maQnZ1NSEjeR+NWqxV/f38yMzM9Eqy8ym2ZLWwq2z17XMvc4blEpHQLCgpi5MiRnD59mk6dOqk1VkTEBMW6M+H5558nODhvbFSbzcbf/vY3wsPD3dvefPPNkktXDuX2mS2sm0Guli29lUZEisMwDDZs2EClSpVo1qwZAPXr16e+5p8WETFNkYvZG2+8kT25TYe/6dKlCwcPHnQ/V6vElV3uBrB9+1zL9u29mUhEiiIzM5MFCxawe/du/P39qV27NmFhBUclERER7ypyMbt69WoPxqg4UrJTAAgPCM+33eGACxdc61FRXg4lIpd1/Phx4uPjSU5OxsfHh969e1Op0qU/XREREe/RAIhellvM/r6bwcUTJnTo4M1EInIphmGwbt06VqxYgdPpJDIykri4OGpqvmkRkVJDxayXpeekAwVHMzhwIG/9om7JImISp9PJrFmz2Lt3LwDNmzdn6NChBAQEmJxMREQupmLWy/aec/1iDPbLX7H+/LNrGRjo7UQiUhir1UrlypXx8fFhwIABtGvXTvcFiIiUQipmvcwwDAAcRv7xt3budC1jY70cSETcDMMgOzubwN/+quzTpw9t27alatWqJicTEZFLKfKkCVIyAnxdH1FGBEbk2378uGupYblEzJGens7nn3/O559/juO3wZ59fHxUyIqIlHJXVcz+8MMP3HHHHXTu3JkTJ04A8Mknn7BmzZoSDVcerTnq+hpFBecfsuC3LyMarlLE+w4fPsz777/P/v37OXXqFAkJCWZHEhGRIip2MTtnzhz69+9PUFAQP//8M9nZ2QAkJyfz2muvlXjA8ua6yq65ai3k73uXnOxaNmzo7UQiFZfT6eS7775jxowZpKamEhUVxQMPPECtWrXMjiYiIkVU7GL21VdfZcqUKXz44Yf4+fm5t3ft2pUtW7aUaLjyaF+Sa2aE6JDofNtPnnQtmzf3diKRiiktLY1PP/2U1atXYxgGrVu35oEHHiA6OvrKB4uISKlR7BvA9uzZw4033lhge3h4OBdyR/2XKwr0zRu2wGbL267hK0W8Y968eRw6dAg/Pz8GDx5Mq1atzI4kIiJXodgts9WrV2f//v0Ftq9Zs+aq5yd/9913iY2NJTAwkI4dO7Jhw4YiHTdz5kwsFgvDhw+/quuawdfq+vshLCBvGszcYbkAatf2diKRimngwIHUrl2bBx98UIWsiEgZVuxi9oEHHuCxxx5j/fr1WCwWTp48yWeffcZTTz3Fww8/XOwAs2bNYsKECbz44ots2bKFVq1a0b9/fxITEy973OHDh3nqqafo3r17sa9plhxHDnanHcg/zuyuXXn7WDW+hIhHpKamsn37dvfzqKgo7r33XqI0f7SISJlW7G4Gzz77LE6nk969e5ORkcGNN95IQEAATz31FI8++mixA7z55ps88MAD3HPPPQBMmTKFRYsWMXXqVJ599tlCj3E4HNx+++28/PLL/PDDD2Wme0NGToZ7/eJi9uBB11JjzIp4xv79+5k3bx6ZmZmEhYVRt25dAE2CICJSDhS7mLVYLDz33HP86U9/Yv/+/aSlpdGsWTNCQ0OLfXGbzcbmzZuZOHGie5vVaqVPnz6sW7fukse98sorREdHc9999/HDDz9c9hrZ2dnuERcAUlJSip2zpOQWs1aLFX8ff/f23F4bMTFmpBIpv5xOJytXrmTt2rWAq5vU1fysEhGR0uuqZwDz9/enWbNm13Txs2fP4nA4qFatWr7t1apVY/fu3YUes2bNGj7++GO2bt1apGtMmjSJl19++ZpylpTcYjbELyRfi9Dp066lRjIQKTnJycnMmTOHY8eOAdC+fXv69++Pr68mPhQRKU+K/VO9V69el/1obuXKldcU6HJSU1P5wx/+wIcffljkfm4TJ05kwoQJ7ucpKSnEmNQEmp6TDuTvYgB5EybUq+ftRCLl0969e5k/fz6ZmZkEBAQwdOhQmuuvRRGRcqnYxWzr1q3zPc/JyWHr1q3s2LGDu+66q1jnioqKwsfHh9O5TZO/OX36NNWrVy+w/4EDBzh8+DBDhw51b3M6nQD4+vqyZ88eGjRokO+YgIAAAgICipXLU3JbZn9fzO7Z41qqz6xIyUhOTiYzM5MaNWoQFxdH5cqVzY4kIiIeUuxi9t///neh21966SXS0tKKdS5/f3/atWvHihUr3MNrOZ1OVqxYwfjx4wvs36RJk3x3IwP85S9/ITU1lbfeesu0FteiKqyY/a0WBzT7l8i1MAzD/alR+/bt8fPzo0WLFupWICJSzpXYT/k77riDDh068MYbbxTruAkTJnDXXXfRvn17OnTowOTJk0lPT3ePbnDnnXdSq1YtJk2aRGBgIC1atMh3fEREBECB7aVRYcXsuXN5r19jF2SRCmv37t18//333HnnnQQGBmKxWAp8iiQiIuVTiRWz69atIzAw8Mo7/s6YMWM4c+YML7zwAgkJCbRu3ZqlS5e6bwo7evQo1nIy+GphxWxSkmsZFgZX8eUTqdDsdjvffvst69evB+DHH3/kpptuMjmViIh4U7GL2ZEjR+Z7bhgGp06dYtOmTTz//PNXFWL8+PGFdisAWL169WWPnT59+lVd0wyFFbPnz7uW6tInUjxJSUnEx8dz6tQpADp37kyPHj1MTiUiIt5W7GI2PDw833Or1Urjxo155ZVX6NevX4kFK48KK2YPH3YtIyNNCCRSRv366698/fXXZGdnExQUxPDhw2nUqJHZsURExATFKmYdDgf33HMP119/PZGqvoqtsGI2d1iu48fNSCRS9mzevJmFCxcCEBMTQ1xcHGFhYSanEhERsxSrM6qPjw/9+vUrM9PHljaFFbOHDrmWalQSKZqmTZsSFhZGt27duPvuu1XIiohUcMW+s6pFixYcPHjQE1nKvcKK2cRE1/J3k6CJyEVyZ/ECCA4O5v/+7//o3bt3ubk5VERErl6xfxO8+uqrPPXUUyxcuJBTp06RkpKS7yGXdvF0trlOnnQtGzc2I5FI6ZaTk8OCBQuYOnVqvimsS8tEKCIiYr4i95l95ZVXePLJJxk0aBAAN998c75pbXMHLHc4HCWfspxItxWcztbf37WsUcOMRCKl15kzZ4iPjyfxt48vUlNTTU4kIiKlUZGL2ZdffpmHHnqIVatWeTJPuZZhL9jNILf7sWb/Esnzyy+/sGjRInJycggJCWHkyJHUr1/f7FgiIlIKFbmYNQwDQOM4XoND5113exU2zqwGhxABm83GkiVL3F0K6tevz4gRIwgNDTU3mIiIlFrFGprr4m4FUnxptjQADAz3No0zK5Ln5MmTbN26FYvFQs+ePenWrZtu8hIRkcsqVjHbqFGjKxa0Sbnzs0oBkUGuijXQ1zVvbXZ23muaAUwEYmNj6devHzVq1CA2NtbsOCIiUgYUq5h9+eWXC8wAJkWXmZMJQFRwFJDXxQCgShUzEomYKzs7m2+++YauXbtS+be/6Dp37mxyKhERKUuKVczeeuutREdHeypLuZdpdxWzuS2zFzdi65NUqWgSEhKIj4/n3LlzJCYmcu+996ork4iIFFuRi1n9krl2uS2zuTeA5XYzUGO3VCSGYbB582aWLl2Kw+EgLCyMvn376meMiIhclWKPZiBX7/cts+fOubari4FUFFlZWSxcuJBff/0VcPXDHzZsGMHBwVc4UkREpHBFLmadTqcnc1QIWfYsAIJ8g4C8MWaPHDEpkIgXnT9/nk8++YTz589jtVrp06cPnTp1UousiIhck2L1mZVrkzudbZCfq5j99lvX9vbtzUok4j1hYWEEBQXhdDqJi4ujdu3aZkcSEZFyQMWslzgNJzaHDcjrM5vbc8NX/wpSTmVlZeHv74/VasXHx4dbbrkFf39/goKCzI4mIiLlhO6h95Jse96gsrl9Ztetcz3v2NGMRCKedeLECd5///18U2CHh4erkBURkRKlYtZLcm/+AgjwCQDy+srWqmVGIhHPMAyDdevWMXXqVC5cuMDOnTux2WxmxxIRkXJKH3B7Se6wXAB+Pn4ApKS4njdvbkYikZKXmZnJ/Pnz2bt3LwDNmjVj6NCh+Pv7m5xMRETKKxWzXpI7kkGof2iB1+rX93YakZJ37Ngx4uPjSUlJwcfHhwEDBtCuXTuNViAiIh6lYtZLckcyyO0vm5nXUEvVqmYkEik5WVlZfPbZZ2RnZ1O5cmVGjx5N9erVzY4lIiIVgIpZL0mzpQFwNuMsAKdO5b2mGcCkrAsMDGTAgAEcPHiQwYMHExAQYHYkERGpIFTMeonDcAAQFhAGwKFDea/pU1gpi44cOYLVaiUmJgaA1q1b06pVK3UrEBERr1Ix6yW5Q3PVCa8D5M3+pVGKpKxxOp2sWbOG1atXExoaykMPPeSejlaFrIiIeJuKWS/JvQEsd1iuM2dc22+6yaxEIsWXlpbGvHnzOHjwIAD169fHV7N+iIiIifRbyEvSc9IBCPEPAfKK2Zo1zUokUjyHDh1izpw5pKen4+fnx6BBg2jdurXZsUREpIJTMesluePM5k5lu2aNa7tGMpDSzjAMVq9ezffffw9AdHQ0cXFxVNU3r4iIlAIqZr0kdwawIF9XJ9kqVVzb09LMSiRSdGfPukbhaNOmDQMHDsTPz8/kRCIiIi4qZr0kt2U2d5zZTZtc26+/3qxEIpdnGAYWiwWLxcLQoUNp3rw5zZo1MzuWiIhIPlazA1QU2Q7XaAYBvq4bwH67+Run06xEIoVzOp18++23xMfHYxgG4BpHVoWsiIiURmqZ9ZLcoblyRzP45RfXdk1lK6VJcnIyc+bM4dixY4BrLNnY2FhzQ4mIiFyGilkv2Ze0D8grZnNFRpqRRqSgvXv3Mn/+fDIzMwkICGDo0KEqZEVEpNRTMeslVYJcd3wlpCeQmZm3vU4dkwKJ/MbhcLBixQrWrVsHQI0aNYiLi6Ny5comJxMREbkyFbNekjudbb2Iepw+nbc9KsqkQCK/mTNnDrt27QKgQ4cO9O3bVxMhiIhImaHfWF6S22e2kn8ljh/P267ZP8VsHTt25MiRIwwdOpQmTZqYHUdERKRYVMx6ybbEbYBrNIMjh1zbVMiKGex2OwkJCdSuXRuAunXr8thjj+Hv729yMhERkeLT0FxeUifc1Tk2MT3RPRyXbv4Sbzt//jxTp05lxowZnMmdUxlUyIqISJmlllkvsTlsAFxX+Tr2fOfa1r27iYGkwtm5cycLFiwgOzuboKAg0tLSNCWtiIiUeSpmvcQ9zqxvAEeOuLalp5sYSCoMu93OsmXL2PTbtHMxMTGMGjWK8PBwk5OJiIhcOxWzXvJzws+Aa5zZ/ftd2+rWNTGQVAjnzp0jPj6ehIQEALp27UqvXr3w8fExOZmIiEjJUDHrJXXD65KYnkhGTgZhYa5t1aqZm0nKv23btpGQkEBwcDAjRoygYcOGZkcSEREpUSpmvSTHmQNAjUo12L7dta1lSxMDSYXQo0cPbDYbnTt3Jiz3rygREZFyRKMZeEnuDWABPgHY7a5tgYEmBpJy6ezZs8yfPx/7b99kVquV/v37q5AVEZFySy2zXrLzzE7AdQNY7ohImspWStIvv/zCokWLyMnJISwsjJtuusnsSCIiIh6nYtZLQv1DSbOlYc/JmymhenUTA0m5YbPZWLJkCVu3bgWgXr16dOjQwdxQIiIiXqJi1ksCfAJIIw0yK7u36QYwuVaJiYnEx8dz5swZLBYLPXr0oHv37lit6kEkIiIVg4pZL8myZwFgzwoCIDgYVG/Itdi9ezdz5szBbrcTGhrKqFGjiI2NNTuWiIiIV6mY9ZLcYjblnKuYzcw0M42UB9HR0fj4+FC3bl1GjBhBSEiI2ZFERES8TsWsF9iddhyGA4ALZ11DGEREmBhIyqz09HR30Vq5cmXuu+8+oqKisFgsVzhSRESkfNIH3V6Q2yoLkJkWAIBhmJVGyiLDMNi0aROTJ0/mwIED7u1Vq1ZVISsiIhWaWma94OJiNjnJ9SVv3dqkMFLmZGVlsXDhQn799VcAduzYQYMGDUxOJSIiUjqomPWCzBxXB1l/H398fNQYLkV38uRJ4uPjOX/+PFarld69e9O5c2ezY4mIiJQaKma9ILdlNsAngMOHXdsaNzYvj5R+hmGwYcMGli9fjsPhIDw8nLi4OGrXrm12NBERkVJFxawXZDuyAQj0DSTrtx4HNpuJgaTUO3ToEEuXLgWgSZMm3HzzzQQFBZmcSkREpPRRMesF2XZXMRvgG8C2ba5tkZEmBpJSr379+rRt25bo6Gg6dOigm7xEREQuQcWsF+S2zAb4BBAd7dqmIUHlYrmjFTRv3pzg4GAAhg4danIqERGR0k93I3nBxS2zq1a5tl1/vYmBpFTJyMhg5syZLF68mPnz52No3DYREZEiU8usF9gcrg6y/j7+VK0KR4+Cn5/JoaRUOHbsGPHx8aSkpODj48N1111ndiQREZEyRcWsF1xczB496toWE2NiIDGdYRisXbuWlStXYhgGlStXZvTo0VSvXt3saCIiImWKilkvuLiYDQ2FtDQIDTU5lJgmIyODefPmsX//fgBatGjBkCFDCAgIMDmZiIhI2aNi1gtyx5n1twaQlubaFhZmYiAxldVq5ezZs/j6+jJw4EDatGmj0QpERESukopZL8htmfXJiXBvq1TJpDBiitybuiwWC4GBgdxyyy1YrVaqVatmcjIREZGyTaMZeEFuMUt6Vfc2dTOoONLS0vj000/ZtGmTe1uNGjVUyIqIiJQAtcx6QW4xa7Gpb0FFc+jQIebMmUN6ejqnTp2iZcuW6hsrIiJSglTMekHupAnZSa471Rs2NDONeIPT6eS7777j+++/B6Bq1aqMHj1ahayIiEgJUzHrBbkts764CpnfbmKXcio1NZW5c+dy+PBhANq0acPAgQPx0+DCIiIiJU7FrBe4+8zmuKYp7d3bxDDiUTabjQ8++IC0tDT8/PwYMmQILVu2NDuWiIhIuaVi1gtyHDkAZJyJAsDHx8w04kn+/v7ccMMN7Ny5k9GjR1OlShWzI4mIiJRrKma9ILdlNjDIAUBioplppKSlpKSQk5PjLly7detGly5d8PXVfy8RERFP09BcXpDjdLXMJu6vA0D79mamkZK0d+9epkyZwpdffklOjuvf2Wq1qpAVERHxEv3G9YLcbgahEa6ZwNQyW/Y5HA5WrFjBunXrAIiIiCAzM1M3eYmIiHiZilkvyG2ZPX/S1WdWLbNl24ULF5gzZw7Hjx8HoEOHDvTt21etsSIiIiYoFd0M3n33XWJjYwkMDKRjx45s2LDhkvt++OGHdO/encjISCIjI+nTp89l9y8NcotZR7ar1c5uNzONXIvdu3fz/vvvc/z4cQICArjlllsYOHCgClkRERGTmF7Mzpo1iwkTJvDiiy+yZcsWWrVqRf/+/Um8xGfxq1ev5rbbbmPVqlWsW7eOmJgY+vXrx4kTJ7ycvOhyuxn4BzgBiIw0M41cLcMwWLduHVlZWdSsWZNx48bRtGlTs2OJiIhUaBbDMAwzA3Ts2JEbbriBd955B3DNnBQTE8Ojjz7Ks88+e8XjHQ4HkZGRvPPOO9x5551X3D8lJYXw8HCSk5MJC/PO9LK3xt/KrF9nEfSvTDJTA5k5E8aM8cqlpYQlJyezadMmevbsiY/GWBMREfGI4tRrprbM2mw2Nm/eTJ8+fdzbrFYrffr0cd9YcyUZGRnk5ORQuXLlQl/Pzs4mJSUl38PbNp3cBEBUrVQAHA6vR5CrtHPnTlatWuV+Hh4eTu/evVXIioiIlBKmFrNnz57F4XBQrVq1fNurVatGQkJCkc7xzDPPULNmzXwF8cUmTZpEeHi4+xETE3PNuYuraVXXR9HHdlcFoE4dr0eQYrLb7SxatIjZs2fz/fffc+jQIbMjiYiISCFM7zN7Lf7+978zc+ZM5s2bR2BgYKH7TJw4keTkZPfj2LFjXk5ZcGiuoCCvR5BiOHfuHB9//DGbNrla1Lt27Uod/QUiIiJSKpl6C3ZUVBQ+Pj6cPn063/bTp09TvXr1yx77xhtv8Pe//51vv/2Wli1bXnK/gIAAAgICSiTv1codzSDtgqvgrlrVzDRyOdu3b2fhwoXYbDaCg4MZMWIEDRs2NDuWiIiIXIKpLbP+/v60a9eOFStWuLc5nU5WrFhB586dL3ncP//5T/7617+ydOlS2peBQVtzHDlgy2uODQ83MYxc0rJly5g7dy42m426desybtw4FbIiIiKlnOmDY06YMIG77rqL9u3b06FDByZPnkx6ejr33HMPAHfeeSe1atVi0qRJAPzjH//ghRde4PPPPyc2NtbdtzY0NJTQ0FDT3sfl5DhzILOK+7mXBlGQYqpduzYA3bt3p2fPnlitZboXjoiISIVgejE7ZswYzpw5wwsvvEBCQgKtW7dm6dKl7pvCjh49mq+o+O9//4vNZiMuLi7feV588UVeeuklb0YvMrvTDrYQ93OLxcQwkk9aWpr7j6DmzZtTrVo1oqKiTE4lIiIiRWX6OLPeZsY4s23eb8PWnw14fys1asDJk165rFyGzWZjyZIl7Nu3j4ceeqjUtuqLiIhURMWp10xvma0IHE4HZLru+tJIBuZLTEwkPj6eM2fOYLFYOHjw4GVvIhQREZHSS8WsF9iddjBcXSUOHjQ5TAVmGAZbt25l8eLF2O12QkNDGTVqFLGxsWZHExERkaukYtYLHIYDslxDGHTqZHKYCspms7Fw4UK2b98OQIMGDRgxYgQhISFXOFJERERKMxWzXmB32iHFNfOYZkE1x/fff8/27duxWCz06tWLbt26YdGdeCIiImWeilkvcDgdkBUBQHq6uVkqqhtvvJFTp07Ro0cPzeYlIiJSjmggTS+wO+3ubgb165scpoLIzs7mxx9/JHewDn9/f/7whz+okBURESln1DLrBXanHc42BSAiwtwsFcGpU6eIj48nKSkJgC5dupicSERERDxFxawX2J12CEkEwN/f5DDlmGEYbNy4kW+++QaHw0F4eLhaYkVERMo5FbNeYHfa4bhrGINWrUwOU05lZWWxYMECdu3aBUDjxo0ZNmwYQRrYV0REpFxTMesFdqcdwo5D0nVkZpqdpvw5efIks2fP5sKFC1itVvr27UvHjh01WoGIiEgFoGLWC+xOO9gDAd0A5gmGYZCSkkJERARxcXHUqlXL7EgiIiLiJSpmvSDHmQPHOwMQHGxymHLC6XRitboG46hVqxZjxoyhTp06BAYGmpxMREREvElDc3mY03DiNJwQkAzoBrCScOzYMd577z0SEhLc2xo1aqRCVkREpAJSMethDqfDtZLtGmc2OtrEMGWcYRisXbuWadOmce7cOVauXGl2JBERETGZuhl4mN1pBwOwOMDwITzc7ERlU3p6OvPnz2f//v0AtGjRgiFDhpicSkRERMymYtbD7E47OH3B8AFAI0UV35EjR5gzZw6pqan4+voyYMAA2rZtq9EKRERERMWspzkMh3skAwB16yyeo0eP8r///Q/DMKhSpQqjR4+mWrVqZscSERGRUkLFrIfZnXbIruR+HhBgYpgyqHbt2sTGxlKpUiUGDx6Mv+6gExERkYuomPUwh9MBtlAArFbXQy7v6NGj1KhRAz8/P6xWK7fddht+fn5mxxIREZFSSKWVh9mddsiKAMDpNDdLaed0Olm9ejXTpk1j2bJl7u0qZEVERORS1DLrYQ7DAYbrbwbVZJeWmprK3LlzOXz4MAAOhyPfxAgiIiIihVEx62F2px0cro6yDRqYHKaUOnDgAHPnziUjIwM/Pz+GDBlCy5YtzY4lIiIiZYCKWQ9zOPNGM9DNX/k5nU5WrVrFmjVrAKhWrRpxcXFERUWZnExERETKChWzHuYwHJDumvbLbjc5TCmTnp7O5s2bAWjXrh39+/dX/1gREREpFhWzHmZ32sEnG4CzZ00OU8pUqlSJ4cOHY7PZaNGihdlxREREpAxSMethF3czaNXK5DAmczgcrFy5kjp16tC4cWMAGjVqZHIqERERKct0q7iHOQwHpNQGKvbsX8nJyUyfPp0ff/yRr776iqysLLMjiYiISDmgllkPczgdYHUAcPq0yWFMsmfPHubPn09WVhYBAQEMHTqUwIpc2YuIiEiJUTHrYXZn3l1fNWqYGMQEDoeD5cuXs379egBq1qxJXFwckZGRJicTERGR8kLFrIc5DAc4/AGIjjY5jBfl5OQwffp0Tp48CUCnTp3o06cPPj4+JicTERGR8kTFrIc5nA5IqwZUrHFm/fz8qF69OklJSQwfPtx9w5eIiIhISVIx62F2px2yIgDIzjY3i6fZ7XZycnIICgoCYMCAAdx4442Eh4ebnExERETKK41m4GFOwwmBFwBwOMzN4klJSUl8/PHHzJ49G6fTCbhaZ1XIioiIiCepZdbDLu4zGxtrbhZP2bFjB19//TU2m42goCDOnz9PlSpVzI4lIiIiFYCKWQ9zGk53Mevvb3KYEpaTk8PSpUvZsmULAHXq1GHUqFGEhYWZnExEREQqChWzHuY0nHCiA1C+itmzZ88SHx/P6d8Gz+3evTs9e/bEalXPFREREfEeFbMe5nA6oMo+ONuMpCSz05QMwzCYO3cup0+fJjg4mJEjR9KgQQOzY4mIiEgFpGLWw5yGE3JCALjuOpPDlBCLxcLNN9/MihUruPnmm6lUqZLZkURERKSC0mfCHuYwHHCsKwC/jVhVJiUmJrJt2zb38+rVq3P77berkBURERFTqWXWw5yGE6J2w6l22Gxmpyk+wzDYunUrixcvxul0UqVKFWrVqmV2LBERERFAxazHOZwOsAcCUKOGyWGKyWazsWjRIneLbP369YmIiDA3lIiIiMhFVMx6WGJ6IpxpDkBgoMlhiuH06dPMnj2bc+fOYbFY6NWrF926dcNisZgdTURERMRNxayHhQWEQchpSK9GWRm1asuWLSxevBiHw0GlSpUYNWoUdevWNTuWiIiISAEqZj3MYrGA0/VlLitzCWRlZeFwOGjYsCEjRowgODjY7EgiIiIihVIx62FlZQYwp9PpnvCgc+fOhIeH06xZM3UrEBEpJxwOBzk5OWbHEHHz9/cvkcmWVMx6mMPpAIcfAH5+JocphGEYbNy4kS1btnDvvffi7++PxWKhefPmZkcTEZESYBgGCQkJXLhwwewoIvlYrVbq1auH/zW29qmY9TCH0wkO151fpa2YzcrKYsGCBezatQtw9ZXt1KmTyalERKQk5Ray0dHRBAcH6xM3KRWcTicnT57k1KlT1KlT55q+L1XMeliO3eleL03F7IkTJ4iPj+fChQtYrVb69u1Lx44dzY4lIiIlyOFwuAvZKlWqmB1HJJ+qVaty8uRJ7HY7ftdQJKmY9TCbLe8vjdIwA5hhGKxfv57ly5fjdDqJiIggLi5OEyGIiJRDuX1kdSOvlEa53QscDoeK2dIsOyuvmA0IMDHIb77//ntWr14NQNOmTbn55psJLEsD4IqISLGpa4GURiX1fali1sNs2a5/KIvVia+v+QPNtmvXjp9//pkuXbpwww036AeciIiIlGnmV1flXGam60tsOM35UhuGwYEDB9zPQ0NDGT9+PB06dFAhKyIiFUZsbCyTJ082O4Z4gIpZD7Pbf1uxOC+7nydkZGTwxRdf8Omnn/Lrr7+6t/v6qkFeRERKJ4vFctnHSy+9dFXn3bhxIw8++GCJZPziiy/w8fHhkUceKfDa9OnTiYiIKPQ4i8XC/Pnz822bM2cOPXv2JDw8nNDQUFq2bMkrr7xCUlJSiWQtTFJSErfffjthYWFERERw3333kZaWdtljDhw4wIgRI6hatSphYWHccsstnD592v366tWrL/lvtnHjRo+9F1Ax63FHzp0CICgixbvXPXKEKVOmsG/fPnx8fDRQtoiIlAmnTp1yPyZPnkxYWFi+bU899ZR7X8MwsLtbjS6vatWqJXYj3Mcff8zTTz/NF198QVZW1lWf57nnnmPMmDHccMMNLFmyhB07dvCvf/2LX375hU8++aREshbm9ttv59dff2X58uUsXLiQ77///rKFfnp6Ov369cNisbBy5UrWrl2LzWZj6NChOJ2uxrouXbrk+3c6deoU999/P/Xq1aN9+/Yeey+gPrMeVzXANUqAw5IJRHj8eoZhsGbNGlatWoVhGFSpUoXRo0dTrVo1j19bRERKP8MwyMjJ8Pp1g/2KNsZt9erV3evh4eFYLBb3ttWrV9OrVy8WL17MX/7yF7Zv384333xDTEwMEyZM4KeffiI9PZ2mTZsyadIk+vTp4z5XbGwsjz/+OI8//jjgaiX98MMPWbRoEcuWLaNWrVr861//4uabb75svkOHDvHjjz8yZ84cVq1axdy5cxk7dmyxvx4bNmzgtddeY/LkyTz22GP5cvbt29djk1zs2rWLpUuXsnHjRneR+Z///IdBgwbxxhtvULNmzQLHrF27lsOHD/Pzzz8TFhYGwP/+9z8iIyNZuXIlffr0wd/fP9+/XU5ODl999RWPPvqox7s1qpj1sMw01xAGgQGebwRPT09n7ty5HDx4EICWLVsyePDga55ZQ0REyo+MnAxCJ4V6/bppE9MI8Q8pkXM9++yzvPHGG9SvX5/IyEiOHTvGoEGD+Nvf/kZAQAAzZsxg6NCh7Nmzhzp16lzyPC+//DL//Oc/ef311/nPf/7D7bffzpEjR6hcufIlj5k2bRqDBw8mPDycO+64g48//viqitnPPvuM0NBQ/u///q/Q1y/VVQGgefPmHDly5JKvd+/enSVLlhT62rp164iIiMjXWtqnTx+sVivr169nxIgRBY7Jzs7GYrEQcNGwTIGBgVitVtasWZPvj4ZcCxYs4Ny5c9xzzz2XzFlSVMx6mMXqan5POeX5ltETJ05w8OBBfH19GTRoEK1bt9ZNXiIiUu688sor9O3b1/28cuXKtGrVyv38r3/9K/PmzWPBggWMHz/+kue5++67ue222wB47bXXePvtt9mwYQMDBgwodH+n08n06dP5z3/+A8Ctt97Kk08+yaFDh6hXr16x3sO+ffuoX7/+VY2vunjx4st2Hwy6zMD2CQkJREdH59vm6+tL5cqVSUhIKPSYTp06ERISwjPPPMNrr72GYRg8++yzOBwOTp06VegxH3/8Mf3796d27dpFeEfXRsWshzmdrmKyct0TgGcnJmjUqBH9+vWjQYMGBb5RRUREwPVxf9rEy9/s46nrlpTf98FMS0vjpZdeYtGiRZw6dQq73U5mZiZHjx697HlatmzpXg8JCSEsLIzExMRL7r98+XLS09MZNGgQAFFRUfTt25epU6fy17/+tVjvwTCMYu1/sbp16171sVejatWqzJ49m4cffpi3334bq9XKbbfdRtu2bbFaC37yfPz4cZYtW8aXX37plXwqZj3M4XAtrT6OEj93amoqS5YsoX///oSHhwPQuXPnEr+OiIiUHxaLpcQ+7jdLSEj+/E899RTLly/njTfeoGHDhgQFBREXF4fNZrvseX7fKmqxWNw3NBXm448/JikpKV/Lp9PpZNu2bbz88stYrVbCwsJIT0/H6XTmK/Ry+8Dm/r5u1KgRa9asIScnp9its9fSzaB69eoFCna73U5SUlK+Pq+/169fPw4cOMDZs2fx9fUlIiKC6tWrU79+/QL7Tps2jSpVqlyx/3FJUTHrYbnFrMXn6v8CK8yBAweYN28e6enp2Gw27rjjjhI9v4iISFmxdu1a7r77bnd/z7S0NA4fPlyi1zh37hxfffUVM2fOpHnz5u7tDoeDbt268c033zBgwAAaN26M3W5n69attG3b1r3fli1bAFcRCzB27Fjefvtt3nvvvXw3gOW6cOHCJfvNXks3g86dO3PhwgU2b95Mu3btAFi5ciVOp5OOHTte+gvwm6ioKPcxiYmJBQpWwzCYNm0ad9555zVNUVscKmY9zOlw/VVmtZbMOLNOp5PVq1fzww8/ABAdHX3Jvj0iIiIVwXXXXcfcuXMZOnQoFouF559//rItrFfjk08+oUqVKtxyyy0F7kcZNGgQH3/8MQMGDKB58+b069ePe++9l3/961/Ur1+fPXv28PjjjzNmzBhq1XJ1OezYsSNPP/00Tz75JCdOnGDEiBHUrFmT/fv3M2XKFLp161ZokQvX1s2gadOmDBgwgAceeIApU6aQk5PD+PHjufXWW90jGZw4cYLevXszY8YMOnToALhaW5s2bUrVqlVZt24djz32GE888QSNGzfOd/6VK1dy6NAh7r///qvOWFwqZj3M6XB9w1t9rv0/VUpKCnPmzHH3AWrbti0DBgzw2l8+IiIipdGbb77JvffeS5cuXYiKiuKZZ54hJaVkx3efOnUqI0aMKPTG6lGjRvGHP/yBs2fPEhUVxaxZs3jxxRcZN24cJ0+epHbt2owYMYLnn38+33H/+Mc/aNeuHe+++y5TpkzB6XTSoEED4uLiuOuuu0o0/8U+++wzxo8fT+/evbFarYwaNYq3337b/XpOTg579uwhIyNvCLc9e/YwceJEkpKSiI2N5bnnnuOJJ54ocO6PP/6YLl260KRJE4/l/z2LcS09kMuglJQUwsPDSU5Odo+V5kkjX5jJvL/eSs0W+zmxveFVnychIYEZM2aQmZmJv78/Q4cOpUWLFiWYVEREypusrCz3nfaBgYFmxxHJ53Lfn8Wp19Qy62FZaa4xXq+1m0GVKlWoVKkS4eHhxMXFUaVKlZKIJyIiIlKmqZj1sNxuBhkXKhX72NTUVEJDQ7FYLPj5+TF27FhCQkLw9dU/m4iIiAiA56elquAsv/WVDQjOLtZxe/bs4b333nPf6AWu4TxUyIqIiIjkUTHrYY4c15c4NCq5aPs7HCxbtoyZM2eSlZXFvn37SvyOTBEREZHyQs18HpaV7urQ7ON35UkTzp8/z5w5czhx4gTgGrajb9++hc6uISIiIiIqZj0u/YJrlhKn3eey++3atYuvvvqK7OxsAgMDGTZsmFeHtRAREREpi1TMelhgaDoAtvRLz8aRmprKnDlzcDgc1K5dm1GjRl1y1g8RERERyaNi1sOcTlcXgcha5y65T6VKlRgwYABJSUn07t0bH5/Lt+KKiIiIiIuKWQ/LHZrLYs0/N8Wvv/5KRESEe1q79u3bez2biIiISFmnYtbDHL8Vsz4+rmI2JyeHZcuWsXnzZiIiIhg3bpxmZRERERG5SqXiNvl3332X2NhYAgMD6dixIxs2bLjs/rNnz6ZJkyYEBgZy/fXXs3jxYi8lLb7cbgZWH4OzZ8/y8ccfs3nzZgBatGiBv7+/mfFERERKFYvFctnHSy+9dE3nnj9/fpH3HzduHD4+PsyePbvAa3fffTfDhw8vsH316tVYLBYuXLjg3maz2fjnP/9Jq1atCA4OJioqiq5duzJt2jRycnKu4p0UzbZt2+jevTuBgYHExMTwz3/+84rHrFixgi5dulCpUiWqV6/OM888g91ud7++Z88eevXqRbVq1QgMDKR+/fr85S9/8ej7uBLTW2ZnzZrFhAkTmDJlCh07dmTy5Mn079+fPXv2EB0dXWD/H3/8kdtuu41JkyYxZMgQPv/8c4YPH86WLVto0aKFCe/g8nK7GdSOPMkHH3xPTk4OwcHBjBw5kgYNGpicTkREpHQ5deqUe/3/27v3sKiq9Q/g3xlghvvtGJdJVLxAZZbihfCSaSRoF8oKzpGIksQUsyOVmTe8HNEMLTUrswArjqA9WjxJmJqelPRkCmiB4w2yTmDHvADKdeb9/dFhfo0M6KAMDn4/zzN/zNpr7f3ueUHfWay9d1ZWFubOnQutVmtoc3Z2tkgcly5dQmZmJqZPn47U1FQ8+eSTrdpPXV0dwsLCUFhYiIULF2LIkCFwdXXFvn37kJKSgn79+qFv377XN3gAFRUVGDVqFEJDQ/Hee+/h8OHDGD9+PNzd3REfH29yTGFhIcaMGYNZs2bho48+wn/+8x88//zz0Ol0SElJAQDY2dnh6aefRlBQENzd3VFYWIgJEyZAr9cjOTn5up/HVZF2NmjQIElISDC81+l0otFoZPHixSb7R0ZGyoMPPmjUFhwcLBMnTryq4124cEEAyIULF1oftBnufmSHPPLIZzJv3jyZN2+epKenS0VFhUWOTUREN7fq6mopKiqS6upqQ5teL1JVZfmXXm9+/GlpaeLm5mbUtnbtWrnttttErVZLYGCgrF692rCttrZWEhISxMfHR9RqtXTp0kWSk5NFRKRr164CwPDq2rVri8dOT0+Xe+65R86fPy+Ojo5y6tQpo+2xsbESERHRZNzOnTsFgJw7d05ERF5//XVRKpVy8ODBJn3r6uqkqqrqyh9EK7zzzjvi4eEhtbW1hrZXX31VAgMDmx3z2muvyYABA4zasrOzxd7evsXaZdq0aTJ06FCzYzT189nInHqtXZcZ1NXV4cCBAwgNDTW0KZVKhIaGYu/evSbH7N2716g/AISFhTXbv7a2FhUVFUYvS9LplHB2vggRYPjw4YiJiYGLi4tFYyAiImp06RLg7Gz516VL1x57RkYG5s6di0WLFqG4uBjJycmYM2cO1q1bBwBYuXIlsrOzsWHDBmi1WmRkZKBbt24AgP379wMA0tLSUFZWZnjfnA8//BBPPfUU3NzcMHr0aKSnp7c65tDQUPTr16/JNjs7Ozg5OZkcd+rUKTg7O7f4amkmdO/evbj33nuNljM2/uX73LlzJsc03uv+zxwcHFBTU2NYInm548ePIzc3F8OHD282lrbWrssMzpw5A51OB29vb6N2b29vHDlyxOSY8vJyk/3Ly8tN9l+8eDHmz59/fQJuBX9/Bbbv7AXXns6477772i0OIiIia5eUlIRly5Zh7NixAAB/f38UFRVhzZo1iI2NxalTp9CrVy8MHToUCoUCXbt2NYy95ZZbAADu7u7w8fFp8TjHjh3Dvn37sGnTJgDAU089hcTERMyePRsKhcKsmI8dO9aq//81Gg0KCgpa7OPp6dnstvLycvj7+xu1NdZP5eXl8PDwaDImLCwMb731FtavX4/IyEiUl5djwYIFAIyXfwDA4MGDcfDgQdTW1iI+Pt7Qrz20+5rZtvbaa68hMTHR8L6iogJ+fn4WO372qvb7pkJERHQ5R0egqqp9jnstLl68iBMnTiAuLg4TJkwwtDc0NMDNzQ3AHxdlPfDAAwgMDER4eDgeeughjBo1yuxjpaamIiwsDJ06dQIAjBkzBnFxcfj6669x//33m7UvEblyJxNsbW3Rs2fPVo1trVGjRuGNN97A888/j5iYGKjVasyZMwe7d++GUmn8x/ysrCxUVlaisLAQr7zyClJSUjB9+nSLxtuoXYvZTp06wcbGBqdPnzZqP336dLPfmnx8fMzqr1aroVarr0/AREREVk6hAJr5y/YNrep/FfjatWsRHBxstK3xYUNBQUEoKSnBl19+ie3btyMyMhKhoaH49NNPr/o4Op0O69atQ3l5OWxtbY3aU1NTDcWsq6srfvrppybjz58/DxsbG8PygYCAgGb/2tySU6dO4Y477mixz8yZMzFz5kyT25qrlxq3NScxMRHTpk1DWVkZPDw8UFpaitdeew3du3c36tc4MXjHHXdAp9MhPj4eL730Urs8+Kldi1mVSoX+/ftjx44dhttb6PV67NixA1OmTDE5JiQkBDt27MDf//53Q9u2bdsQEhJigYiJiIioPXh7e0Oj0eDkyZOIjo5utp+rqyuioqIQFRWFJ554wvCETU9PT9jZ2UGn07V4nJycHFRWViI/P9+oMPvhhx/w7LPP4vz583B3d0dgYCAyMzNRW1trNGl28OBB+Pv7w87ODgAwbtw4zJw5E/n5+U3WzdbX16Ours7kutlrXWYQEhKCWbNmob6+3hDLtm3bEBgYaHKJwZ8pFApoNBoAwPr16+Hn54egoKBm++v1etTX10Ov17fPU0zNvvTsOsvMzBS1Wi3p6elSVFQk8fHx4u7uLuXl5SIiEhMTIzNmzDD0z8vLE1tbW0lJSZHi4mJJSkoSOzs7OXz48FUdz9J3MyAiImovLV0tbg0uv5vB2rVrxcHBQVasWCFarVYOHTokqampsmzZMhERWbZsmfzzn/+U4uJi0Wq1EhcXJz4+PqLT6UREpFevXjJp0iQpKyuTs2fPmjxmRESEREVFNWnX6XTi4+Mjb7/9toiInDt3Try8vCQyMlK+//57OXbsmHz44Yfi4uIi7777rmFcTU2NDBs2TDw8POTtt9+WgoICOXHihGRlZUlQUJDk5+dfp0/L2Pnz58Xb21tiYmLkhx9+kMzMTHF0dJQ1a9YY+mzatKnJ3Q2WLl0qhw4dkh9++EEWLFggdnZ2snnzZsP2Tz75RLKysqSoqMhwHhqNRqKjo82O8XrdzaDdi1kRkVWrVkmXLl1EpVLJoEGDZN++fYZtw4cPl9jYWKP+GzZskICAAFGpVNK7d2/ZsmXLVR+LxSwREd0sOloxKyKSkZEhffv2FZVKJR4eHnLvvffKpk2bRETk/fffl759+4qTk5O4urrK/fffb3RLrOzsbOnZs6fY2tqavDVXeXm52NrayoYNG0zGM2nSJOnXr5/hvVarlccee0w0Go04OTnJ3XffLWvXrhX9Zfchq6mpkcWLF0ufPn3E3t5ePD09ZciQIZKeni719fWt/HSurLCwUIYOHSpqtVpuvfVWWbJkidH2tLQ0uXxec8SIEeLm5ib29vYSHBwsOTk5RtszMzMlKChInJ2dxcnJSe644w5JTk5u1c/Y9SpmFSKtXJlspSoqKuDm5oYLFy7A1dW1vcMhIiJqMzU1NSgpKYG/vz8fnU43nJZ+Ps2p126Ix9kSEREREbUGi1kiIiIislosZomIiIjIarGYJSIiIiKrxWKWiIiog7vJrvUmK3G9fi5ZzBIREXVQjTfLv3TpUjtHQtRUXV0dAFzzgxba9QlgRERE1HZsbGzg7u6O3377DQDg6OgIhULRzlER/fHUsP/+979wdHQ0emxwa7CYJSIi6sB8fHwAwFDQEt0olEolunTpcs1fsFjMEhERdWAKhQK+vr7w8vJCfX19e4dDZKBSqaBUXvuKVxazRERENwEbG5trXptIdCPiBWBEREREZLVYzBIRERGR1WIxS0RERERW66ZbM9t4g96Kiop2joSIiIiITGms067mwQo3XTFbWVkJAPDz82vnSIiIiIioJZWVlXBzc2uxj0Jusmfc6fV6/Prrr3BxcbHIjaMrKirg5+eHn3/+Ga6urm1+PLr+mEPrxxxaP+bQujF/1s/SORQRVFZWQqPRXPH2XTfdzKxSqUTnzp0tflxXV1f+Als55tD6MYfWjzm0bsyf9bNkDq80I9uIF4ARERERkdViMUtEREREVovFbBtTq9VISkqCWq1u71ColZhD68ccWj/m0Loxf9bvRs7hTXcBGBERERF1HJyZJSIiIiKrxWKWiIiIiKwWi1kiIiIislosZomIiIjIarGYvQ5Wr16Nbt26wd7eHsHBwfjuu+9a7L9x40bcdtttsLe3R58+fZCTk2OhSKk55uRw7dq1GDZsGDw8PODh4YHQ0NAr5pzanrm/h40yMzOhUCjw6KOPtm2AdEXm5vD8+fNISEiAr68v1Go1AgIC+O9pOzI3f2+99RYCAwPh4OAAPz8/TJs2DTU1NRaKli73zTff4OGHH4ZGo4FCocBnn312xTG7du1CUFAQ1Go1evbsifT09DaP0ySha5KZmSkqlUpSU1Plxx9/lAkTJoi7u7ucPn3aZP+8vDyxsbGRpUuXSlFRkcyePVvs7Ozk8OHDFo6cGpmbw3Hjxsnq1aslPz9fiouL5ZlnnhE3Nzf55ZdfLBw5NTI3h41KSkrk1ltvlWHDhklERIRlgiWTzM1hbW2tDBgwQMaMGSN79uyRkpIS2bVrlxQUFFg4chIxP38ZGRmiVqslIyNDSkpKZOvWreLr6yvTpk2zcOTUKCcnR2bNmiWbNm0SALJ58+YW+588eVIcHR0lMTFRioqKZNWqVWJjYyO5ubmWCfhPWMxeo0GDBklCQoLhvU6nE41GI4sXLzbZPzIyUh588EGjtuDgYJk4cWKbxknNMzeHl2toaBAXFxdZt25dW4VIV9CaHDY0NMjgwYPlgw8+kNjYWBaz7czcHL777rvSvXt3qaurs1SI1AJz85eQkCAjR440aktMTJQhQ4a0aZx0da6mmJ0+fbr07t3bqC0qKkrCwsLaMDLTuMzgGtTV1eHAgQMIDQ01tCmVSoSGhmLv3r0mx+zdu9eoPwCEhYU125/aVmtyeLlLly6hvr4enp6ebRUmtaC1OVywYAG8vLwQFxdniTCpBa3JYXZ2NkJCQpCQkABvb2/ceeedSE5Ohk6ns1TY9D+tyd/gwYNx4MABw1KEkydPIicnB2PGjLFIzHTtbqR6xtbiR+xAzpw5A51OB29vb6N2b29vHDlyxOSY8vJyk/3Ly8vbLE5qXmtyeLlXX30VGo2myS81WUZrcrhnzx58+OGHKCgosECEdCWtyeHJkyfx9ddfIzo6Gjk5OTh+/DgmT56M+vp6JCUlWSJs+p/W5G/cuHE4c+YMhg4dChFBQ0MDnn/+ecycOdMSIdN10Fw9U1FRgerqajg4OFgsFs7MEl2DJUuWIDMzE5s3b4a9vX17h0NXobKyEjExMVi7di06derU3uFQK+n1enh5eeH9999H//79ERUVhVmzZuG9995r79DoKuzatQvJycl45513cPDgQWzatAlbtmzBwoUL2zs0skKcmb0GnTp1go2NDU6fPm3Ufvr0afj4+Jgc4+PjY1Z/alutyWGjlJQULFmyBNu3b8ddd93VlmFSC8zN4YkTJ1BaWoqHH37Y0KbX6wEAtra20Gq16NGjR9sGTUZa83vo6+sLOzs72NjYGNpuv/12lJeXo66uDiqVqk1jpv/XmvzNmTMHMTExeO655wAAffr0wcWLFxEfH49Zs2ZBqeRc242uuXrG1dXVorOyAGdmr4lKpUL//v2xY8cOQ5ter8eOHTsQEhJickxISIhRfwDYtm1bs/2pbbUmhwCwdOlSLFy4ELm5uRgwYIAlQqVmmJvD2267DYcPH0ZBQYHh9cgjj2DEiBEoKCiAn5+fJcMntO73cMiQITh+/LjhiwgAHD16FL6+vixkLaw1+bt06VKTgrXxi4mItF2wdN3cUPWMxS8562AyMzNFrVZLenq6FBUVSXx8vLi7u0t5ebmIiMTExMiMGTMM/fPy8sTW1lZSUlKkuLhYkpKSeGuudmZuDpcsWSIqlUo+/fRTKSsrM7wqKyvb6xRueubm8HK8m0H7MzeHp06dEhcXF5kyZYpotVr54osvxMvLS/7xj3+01ync1MzNX1JSkri4uMj69evl5MmT8tVXX0mPHj0kMjKyvU7hpldZWSn5+fmSn58vAGT58uWSn58vP/30k4iIzJgxQ2JiYgz9G2/N9corr0hxcbGsXr2at+ayZqtWrZIuXbqISqWSQYMGyb59+wzbhg8fLrGxsUb9N2zYIAEBAaJSqaR3796yZcsWC0dMlzMnh127dhUATV5JSUmWD5wMzP09/DMWszcGc3P47bffSnBwsKjVaunevbssWrRIGhoaLBw1NTInf/X19TJv3jzp0aOH2Nvbi5+fn0yePFnOnTtn+cBJRER27txp8v+2xrzFxsbK8OHDm4zp27evqFQq6d69u6SlpVk8bhERhQjn84mIiIjIOnHNLBERERFZLRazRERERGS1WMwSERERkdViMUtEREREVovFLBERERFZLRazRERERGS1WMwSERERkdViMUtEREREVovFLBERgPT0dLi7u7d3GK2mUCjw2WeftdjnmWeewaOPPmqReIiILIXFLBF1GM888wwUCkWT1/Hjx9s7NKSnpxviUSqV6Ny5M5599ln89ttv12X/ZWVlGD16NACgtLQUCoUCBQUFRn1WrFiB9PT063K85sybN89wnjY2NvDz80N8fDzOnj1r1n5YeBPR1bJt7wCIiK6n8PBwpKWlGbXdcsst7RSNMVdXV2i1Wuj1ehQWFuLZZ5/Fr7/+iq1bt17zvn18fK7Yx83N7ZqPczV69+6N7du3Q6fTobi4GOPHj8eFCxeQlZVlkeMT0c2FM7NE1KGo1Wr4+PgYvWxsbLB8+XL06dMHTk5O8PPzw+TJk1FVVdXsfgoLCzFixAi4uLjA1dUV/fv3x/fff2/YvmfPHgwbNgwODg7w8/PD1KlTcfHixRZjUygU8PHxgUajwejRozF16lRs374d1dXV0Ov1WLBgATp37gy1Wo2+ffsiNzfXMLaurg5TpkyBr68v7O3t0bVrVyxevNho343LDPz9/QEA/fr1g0KhwH333QfAeLbz/fffh0ajgV6vN4oxIiIC48ePN7z//PPPERQUBHt7e3Tv3h3z589HQ0NDi+dpa2sLHx8f3HrrrQgNDcWTTz6Jbdu2GbbrdDrExcXB398fDg4OCAwMxIoVKwzb582bh3Xr1uHzzz83zPLu2rULAPDzzz8jMjIS7u7u8PT0REREBEpLS1uMh4g6NhazRHRTUCqVWLlyJX788UesW7cOX3/9NaZPn95s/+joaHTu3Bn79+/HgQMHMGPGDNjZ2QEATpw4gfDwcDz++OM4dOgQsrKysGfPHkyZMsWsmBwcHKDX69HQ0IAVK1Zg2bJlSElJwaFDhxAWFoZHHnkEx44dAwCsXLkS2dnZ2LBhA7RaLTIyMtCtWzeT+/3uu+8AANu3b0dZWRk2bdrUpM+TTz6J33//HTt37jS0nT17Frm5uYiOjgYA7N69G08//TRefPFFFBUVYc2aNUhPT8eiRYuu+hxLS0uxdetWqFQqQ5ter0fnzp2xceNGFBUVYe7cuZg5cyY2bNgAAHj55ZcRGRmJ8PBwlJWVoaysDIMHD0Z9fT3CwsLg4uKC3bt3Iy8vD87OzggPD0ddXd1Vx0REHYwQEXUQsbGxYmNjI05OTobXE088YbLvxo0b5S9/+YvhfVpamri5uRneu7i4SHp6usmxcXFxEh8fb9S2e/duUSqVUl1dbXLM5fs/evSoBAQEyIABA0RERKPRyKJFi4zGDBw4UCZPniwiIi+88IKMHDlS9Hq9yf0DkM2bN4uISElJiQCQ/Px8oz6xsbESERFheB8RESHjx483vF+zZo1oNBrR6XQiInL//fdLcnKy0T4+/vhj8fX1NRmDiEhSUpIolUpxcnISe3t7ASAAZPny5c2OERFJSEiQxx9/vNlYG48dGBho9BnU1taKg4ODbN26tcX9E1HHxTWzRNShjBgxAu+++67hvZOTE4A/ZikXL16MI0eOoKKiAg0NDaipqcGlS5fg6OjYZD+JiYl47rnn8PHHHxv+VN6jRw8AfyxBOHToEDIyMgz9RQR6vR4lJSW4/fbbTcZ24cIFODs7Q6/Xo6amBkOHDsUHH3yAiooK/PrrrxgyZIhR/yFDhqCwsBDAH0sEHnjgAQQGBiI8PBwPPfQQRo0adU2fVXR0NCZMmIB33nkHarUaGRkZ+Otf/wqlUmk4z7y8PKOZWJ1O1+LnBgCBgYHIzs5GTU0NPvnkExQUFOCFF14w6rN69Wqkpqbi1KlTqK6uRl1dHfr27dtivIWFhTh+/DhcXFyM2mtqanDixIlWfAJE1BGwmCWiDsXJyQk9e/Y0aistLcVDDz2ESZMmYdGiRfD09MSePXsQFxeHuro6k0XZvHnzMG7cOGzZsgVffvklkpKSkJmZicceewxVVVWYOHEipk6d2mRcly5dmo3NxcUFBw8ehFKphK+vLxwcHAAAFRUVVzyvoKAglJSU4Msvv8T27dsRGRmJ0NBQfPrpp1cc25yHH34YIoItW7Zg4MCB2L17N958803D9qqqKsyfPx9jx45tMtbe3r7Z/apUKkMOlixZggcffBDz58/HwoULAQCZmZl4+eWXsWzZMoSEhMDFxQVvvPEG/v3vf7cYb1VVFfr372/0JaLRjXKRHxFZHotZIurwDhw4AL1ej2XLlhlmHRvXZ7YkICAAAQEBmDZtGv72t78hLS0Njz32GIKCglBUVNSkaL4SpVJpcoyrqys0Gg3y8vIwfPhwQ3teXh4GDRpk1C8qKgpRUVF44oknEB4ejrNnz8LT09Nof43rU3U6XYvx2NvbY+zYscjIyMDx48cRGBiIoKAgw/agoCBotVqzz/Nys2fPxsiRIzFp0iTDeQ4ePBiTJ0829Ll8ZlWlUjWJPygoCFlZWfDy8oKrq+s1xUREHQcvACOiDq9nz56or6/HqlWrcPLkSXz88cd47733mu1fXV2NKVOmYNeuXfjpp5+Ql5eH/fv3G5YPvPrqq/j2228xZcoUFBQU4NixY/j888/NvgDsz1555RW8/vrryMrKglarxYwZM1BQUIAXX3wRALB8+XKsX78eR44cwdGjR7Fx40b4+PiYfNCDl5cXHBwckJubi9OnT+PChQvNHjc6OhpbtmxBamqq4cKvRnPnzsVHH32E+fPn48cff0RxcTEyMzMxe/Zss84tJCQEd911F5KTkwEAvXr1wvfff4+tW7fi6NGjmDNnDvbv3280plu3bjh06BC0Wi3OnDmD+vp6REdHo1OnToiIiMDu3btRUlKCXbt2YerUqfjll1/MiomIOg4Ws0TU4d19991Yvnw5Xn/9ddx5553IyMgwuq3V5WxsbPD777/j6aefRkBAACIjIzF69GjMnz8fAHDXXXfhX//6F44ePYphw4ahX79+mDt3LjQaTatjnDp1KhITE/HSSy+hT58+yM3NRXZ2Nnr16gXgjyUKS5cuxYABAzBw4ECUlpYiJyfHMNP8Z7a2tli5ciXWrFkDjUaDiIiIZo87cuRIeHp6QqvVYty4cUbbwsLC8MUXX+Crr77CwIEDcc899+DNN99E165dzT6/adOm4YMPPsDPP/+MiRMnYuzYsYiKikJwcDB+//13o1laAJgwYQICAwMxYMAA3HLLLcjLy4OjoyO++eYbdOnSBWPHjsXtt9+OuLg41NTUcKaW6CamEBFp7yCIiIiIiFqDM7NEREREZLVYzBIRERGR1WIxS0RERERWi8UsEREREVktFrNEREREZLVYzBIRERGR1WIxS0RERERWi8UsEREREVktFrNEREREZLVYzBIRERGR1WIxS0RERERW6/8AVCouxLuupU4AAAAASUVORK5CYII=",
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Evaluate and plot results\n",
        "evaluate_and_plot(y_train, y_test, y_train_pred, y_train_pred_proba, y_test_pred, y_test_pred_proba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9V7ohvEVboTV"
      },
      "outputs": [],
      "source": [
        "# # Defining Objective function which will be optimized\n",
        "\n",
        "# def objective(trial):\n",
        "\n",
        "#     # Sampling the classifier type along with its parameter\n",
        "#     classifier = trial.suggest_categorical('classifier', ['RandomForest', 'SVC', 'LogisticRegression', 'XGBoost'])\n",
        "\n",
        "#     if classifier == 'RandomForest':\n",
        "#         # Sampling hyperparameters for RandomForestClassifier\n",
        "#         n_estimators = trial.suggest_int('n_estimators', 50, 300, step=50)\n",
        "#         max_depth = trial.suggest_int('max_depth', 5, 20)\n",
        "#         min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
        "#         min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
        "\n",
        "#         clf = RandomForestClassifier(\n",
        "#             n_estimators=n_estimators,\n",
        "#             max_depth=max_depth,\n",
        "#             min_samples_split=min_samples_split,\n",
        "#             min_samples_leaf=min_samples_leaf\n",
        "#         )\n",
        "\n",
        "#     elif classifier == 'SVC':\n",
        "#         # Sampling hyperparameters for SVC\n",
        "#         C = trial.suggest_loguniform('svc_C', 1e-3, 1e3)\n",
        "#         kernel = trial.suggest_categorical('svc_kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n",
        "#         gamma = trial.suggest_categorical('svc_gamma', ['scale', 'auto'])\n",
        "\n",
        "#         clf = SVC(C=C, kernel=kernel, gamma=gamma)\n",
        "\n",
        "#     elif classifier == 'LogisticRegression':\n",
        "#         # Sampling hyperparameters for LogisticRegression\n",
        "#         C = trial.suggest_loguniform('lr_C', 1e-3, 1e3)\n",
        "#         penalty = trial.suggest_categorical('lr_penalty', ['l1', 'l2'])\n",
        "\n",
        "#         clf = LogisticRegression(C=C, penalty=penalty, solver='liblinear')\n",
        "\n",
        "#     elif classifier == 'XGBoost':\n",
        "#         # Sampling hyperparameters for XGBoostClassifier\n",
        "#         n_estimators = trial.suggest_int('xgb_n_estimators', 50, 300, step=50)\n",
        "#         max_depth = trial.suggest_int('xgb_max_depth', 3, 10)\n",
        "#         learning_rate = trial.suggest_loguniform('xgb_learning_rate', 0.01, 0.1)\n",
        "#         subsample = trial.suggest_uniform('xgb_subsample', 0.6, 0.9)\n",
        "\n",
        "#         clf = XGBClassifier(\n",
        "#             n_estimators=n_estimators,\n",
        "#             max_depth=max_depth,\n",
        "#             learning_rate=learning_rate,\n",
        "#             subsample=subsample\n",
        "#         )\n",
        "\n",
        "#     # experimenting with dropping unnecessary columns and outlier handling\n",
        "#     drop_and_winsorize = trial.suggest_int('drop_and_winsorize',0,1)\n",
        "\n",
        "#     if drop_and_winsorize:\n",
        "#       # experimenting with columns to drop\n",
        "#       columns_to_drop = trial.suggest_categorical('columns_to_drop', ['useless', 'sparse'])\n",
        "#       drop_cols = useless_cols if columns_to_drop=='useless' else sparse_cols\n",
        "#       num_cols_used = [col for col in numeric_cols if col not in drop_cols]\n",
        "#       cat_cols_used = [col for col in cat_cols if col not in drop_cols]\n",
        "#     else:\n",
        "#       drop_cols, num_cols_used, cat_cols_used = [], [], []\n",
        "\n",
        "#     # experimenting with imputers\n",
        "#     add_indicator = trial.suggest_int('add_indicator',0,1)\n",
        "#     imputer = trial.suggest_categorical('imputer',['simple','knn'])\n",
        "\n",
        "#     if imputer == 'knn':\n",
        "#       imp = KNNImputer(weights='distance', add_indicator=add_indicator)\n",
        "#     else:\n",
        "#       imp = SimpleImputer(strategy='median', add_indicator=add_indicator)\n",
        "\n",
        "#     # experimenting with one hot encoding\n",
        "#     keep_first = trial.suggest_int('keep_first',0,1)\n",
        "#     cat_encoder = OneHotEncoder(drop='first') if keep_first else OneHotEncoder()\n",
        "\n",
        "#     # Constructing the numerical pipeline\n",
        "#     num_pipeline = Pipeline([\n",
        "#         ('scaler', StandardScaler()),\n",
        "#         ('imputer', imp),\n",
        "#         ('outlier_clipping', Winsorizer(change=drop_and_winsorize)),\n",
        "#     ])\n",
        "#     # constructing categorical pipeline\n",
        "#     cat_pipeline = Pipeline([\n",
        "#         ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "#         ('onehotencoder', cat_encoder),\n",
        "#     ])\n",
        "\n",
        "#     # constructing training pipeline\n",
        "#     training_pipeline = Pipeline([\n",
        "#         ('Drop_Columns', FunctionTransformer(drop_col, kw_args={'columns': drop_cols, 'drop': drop_and_winsorize})),\n",
        "#         ('Balancing', RandomOverSampler()),\n",
        "#         ('Feature_transform', ColumnTransformer([\n",
        "#             ('num_pipeline', num_pipeline, num_cols_used),\n",
        "#             ('cat_pipeline', cat_pipeline, cat_cols_used),\n",
        "#             ])),\n",
        "\n",
        "#     ('Model training', clf)\n",
        "#     ])\n",
        "\n",
        "#     scorer = make_scorer(f1_score)\n",
        "\n",
        "#     scores = cross_val_score(training_pipeline, X_train, y_train, n_jobs=-1, cv=5, scoring=scorer)\n",
        "#     # report_cross_validation_scores(trial, scores)\n",
        "\n",
        "#     # Returning the cross-validated mean score\n",
        "#     return scores.mean()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-foBm_DLgsz_"
      },
      "outputs": [],
      "source": [
        "# study = optuna.create_study(direction=\"maximize\")\n",
        "# # terminator = TerminatorCallback()\n",
        "# study.optimize(objective, n_trials=100, n_jobs=2, )#, #callbacks=[terminator])\n",
        "# trial = study.best_trial\n",
        "\n",
        "# print('f1_score: {}'.format(trial.value))\n",
        "# print(\"Best hyperparameters: {}\".format(trial.params))\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
